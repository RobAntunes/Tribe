This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
agents/
  __init__.py
commands/
  __init__.py
  agent_commands.py
core/
  __init__.py
  crew_collab.py
  dynamic_flow_generator.py
  dynamic.py
  team_optimizer.py
knowledge/
  user_preference.txt
src/
  python/
    tools/
      __init__.py
      formatting.py
      linting.py
    __init__.py
  __init__.py
tests/
  e2e/
    test_crew_system_e2e.py
  integration/
    test_crew_system.py
  unit/
    test_crew_manager.py
  __init__.py
  conftest.py
  requirements-test.txt
  run_tests.py
  test_agent_project_manager.py
  test_autonomous_crew.py
  test_crew_manager.py
  test_flow_manager.py
  test_learning_system.py
tools/
  agents.py
  base_tool.py
  custom_tools.py
  docs.txt
  dynamic_flow_analyzer.py
  system_tools.py
  tool_manager.py
ui/
  __init__.py
__init__.py
.gitignore
crew.py
extension.py
lambda.ts
main.py
pyproject.toml
requirements.txt

================================================================
Files
================================================================

================
File: agents/__init__.py
================
"""
Tribe agents module - Contains agent-related functionality
"""

================
File: commands/__init__.py
================
"""Commands package for Tribe extension."""
from .agent_commands import AgentCommands

__all__ = ['AgentCommands']

================
File: commands/agent_commands.py
================
"""Commands implementation for Tribe extension using CrewAI."""
import os
import json
import asyncio
import logging
from typing import Dict, List, Optional, Any
from crewai import Agent, Task, Process
from ..core.dynamic import DynamicAgent, DynamicCrew, ProjectManager, ProjectTask
from crewai.tools import BaseTool
from crewai_tools import (
    CodeInterpreterTool,
    DirectorySearchTool,
    FileReadTool,
    GithubSearchTool,
    # SerperDevTool,  # Requires API key
)
from datetime import datetime
from ..extension import get_webview

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AgentCommands:
    """Implementation of agent-related commands for Tribe extension."""
    
    def __init__(self, workspace_path: str):
        """Initialize AgentCommands with workspace path."""
        self.workspace_path = workspace_path
        self.project_manager = ProjectManager()
        self.active_crews: Dict[str, DynamicCrew] = {}
        self.active_agents: Dict[str, DynamicAgent] = {}
        self._setup_default_tools()

    def _setup_default_tools(self):
        """Initialize default tools available to all agents."""
        self.default_tools = [
            CodeInterpreterTool(),
            DirectorySearchTool(),
            FileReadTool(),
            GithubSearchTool(),
            # SerperDevTool(),  # Requires API key
        ]

    async def create_team(self, payload: dict) -> dict:
        """Create a new team using the DynamicCrew and VP of Engineering."""
        try:
            logger.info(f"Creating team for project: {payload.get('description')}")
            
            # Create VP of Engineering first
            try:
                vp = await DynamicAgent.create_vp_engineering(payload.get('description', ''))
                logger.info("VP of Engineering created successfully")
                
                # Verify VP initialization
                if not vp.agent_state.project_context.get('initialization_complete'):
                    raise ValueError("VP of Engineering initialization incomplete")
                
                # Add default tools to VP
                vp.tools = self.default_tools.copy()
                logger.info("Added default tools to VP of Engineering")
                
            except Exception as e:
                logger.error(f"Failed to create VP of Engineering: {str(e)}", exc_info=True)
                return {"error": f"Failed to create VP of Engineering: {str(e)}"}
            
            # Create dynamic crew with the VP
            try:
                dynamic_crew = DynamicCrew(
                    config={
                        'agents': [vp],
                        'tasks': [],
                        'process': Process.hierarchical,
                        'verbose': True,
                        'max_rpm': 60,
                        'share_crew': True,
                        'manager_llm': None,
                        'function_calling_llm': None,
                        'language': 'en',
                        'cache': True,
                        'embedder': None,
                        'full_output': False,
                        'planning': True,
                        'api_endpoint': self.endpoint_url
                    }
                )
                
                # Add VP to crew and verify
                dynamic_crew.add_agent(vp)
                if not dynamic_crew.get_active_agents():
                    raise ValueError("Failed to add VP of Engineering to crew")
                    
                logger.info("Dynamic crew created and VP of Engineering added successfully")
                
            except Exception as e:
                logger.error(f"Failed to create dynamic crew: {str(e)}", exc_info=True)
                return {"error": f"Failed to create team infrastructure: {str(e)}"}
            
            # Store crew for future reference
            crew_id = str(int(asyncio.get_event_loop().time() * 1000))
            self.active_crews[crew_id] = dynamic_crew
            
            # Store VP reference
            self.active_agents[vp.id] = vp
            
            # Return successful response with detailed agent info
            return {
                "crew_id": crew_id,
                "team": {
                    "id": crew_id,
                    "description": payload.get('description', ''),
                    "agents": [{
                        "id": vp.id,
                        "role": vp.role,
                        "status": vp.status,
                        "initialization_complete": vp.agent_state.project_context.get('initialization_complete', False),
                        "tools": [tool.name for tool in vp.tools if hasattr(tool, 'name')]
                    }]
                }
            }
            
        except Exception as e:
            logger.error(f"Error creating team: {str(e)}", exc_info=True)
            return {"error": f"Error creating team: {str(e)}"}

    async def _create_agent_from_spec(self, agent_spec: Dict, team_spec: Dict) -> Optional[DynamicAgent]:
        """Create a DynamicAgent from specification."""
        try:
            # Create agent with enhanced capabilities
            agent = DynamicAgent(
                role=agent_spec.get("role"),
                goal=agent_spec.get("goal"),
                backstory=agent_spec.get("backstory"),
                verbose=True,
                allow_delegation=True,
                tools=self.default_tools.copy()
            )
            
            # Configure agent based on team specification
            await agent.configure_collaboration(team_spec.get("collaboration", {}))
            await agent.setup_learning_system(team_spec.get("learning", {}))
            
            # Set autonomy level
            autonomy_level = agent_spec.get("autonomy_level", 0.5)
            await agent.set_autonomy_level(autonomy_level)
            
            return agent
        except Exception as e:
            logger.error(f"Error creating agent: {str(e)}")
            return None

    async def _create_task_from_spec(self, task_spec: Dict, available_agents: List[DynamicAgent]) -> Optional[ProjectTask]:
        """Create a ProjectTask from specification."""
        try:
            # Find best agent for task
            assigned_agent = self._find_best_agent_for_task(task_spec, available_agents)
            if not assigned_agent:
                return None
            
            # Create task with project management integration
            task = ProjectTask(
                description=task_spec.get("description"),
                expected_output=task_spec.get("expected_output"),
                agent=assigned_agent,
                priority=task_spec.get("priority", 1),
                estimated_duration=task_spec.get("estimated_duration", 1.0),
                required_skills=task_spec.get("required_skills", []),
                max_parallel_agents=task_spec.get("max_parallel_agents", 1),
                min_required_agents=task_spec.get("min_required_agents", 1)
            )
            
            # Add dependencies if specified
            for dep_id in task_spec.get("dependencies", []):
                if dep_id in self.project_manager.tasks:
                    task.add_dependency(self.project_manager.tasks[dep_id])
            
            return task
        except Exception as e:
            logger.error(f"Error creating task: {str(e)}")
            return None

    def _find_best_agent_for_task(self, task_spec: Dict, available_agents: List[DynamicAgent]) -> Optional[DynamicAgent]:
        """Find the most suitable agent for a task based on skills and current load."""
        try:
            # Get required skills
            required_skills = set(task_spec.get("required_skills", []))
            
            # Score each agent
            agent_scores = []
            for agent in available_agents:
                # Calculate skill match
                agent_skills = set(agent.skills)
                skill_match = len(required_skills.intersection(agent_skills)) / len(required_skills) if required_skills else 1.0
                
                # Consider current load
                current_tasks = len([t for t in self.project_manager.tasks.values() if agent.id in t.state.assigned_agents])
                load_factor = 1.0 / (current_tasks + 1)
                
                # Consider autonomy level if specified
                required_autonomy = task_spec.get("required_autonomy", 0.5)
                autonomy_match = 1.0 - abs(agent.autonomy_level - required_autonomy)
                
                # Calculate final score
                score = (skill_match * 0.5) + (load_factor * 0.3) + (autonomy_match * 0.2)
                agent_scores.append((score, agent))
            
            # Return agent with highest score
            if agent_scores:
                return max(agent_scores, key=lambda x: x[0])[1]
            return None
        except Exception as e:
            logger.error(f"Error finding best agent: {str(e)}")
            return None

    async def _execute_crew_tasks(self, crew_id: str):
        """Execute crew tasks asynchronously."""
        try:
            crew = self.active_crews.get(crew_id)
            if not crew:
                return
            
            # Execute tasks with proper error handling
            results = await crew.execute_tasks(crew.tasks)
            
            # Process results
            for result in results:
                if isinstance(result, dict) and "error" in result:
                    logger.error(f"Task execution error: {result['error']}")
                    # Handle task failure
                    await self._handle_task_failure(result, crew)
                else:
                    # Process successful result
                    await self._process_task_result(result, crew)
                    
        except Exception as e:
            logger.error(f"Error executing crew tasks: {str(e)}")

    async def _handle_task_failure(self, error_result: Dict, crew: DynamicCrew):
        """Handle task execution failure."""
        try:
            # Create recovery task
            recovery_task = Task(
                description=f"""Analyze and recover from task failure:
                Error: {error_result.get('error')}
                Task: {error_result.get('task')}
                
                1. Analyze failure cause
                2. Propose recovery strategy
                3. Create compensating tasks if needed""",
                expected_output="Recovery plan and actions taken",
                agent=crew.manager_agent
            )
            
            # Execute recovery
            await crew.execute_tasks([recovery_task])
            
        except Exception as e:
            logger.error(f"Error handling task failure: {str(e)}")

    async def _process_task_result(self, result: Any, crew: DynamicCrew):
        """Process successful task result."""
        try:
            # Create analysis task
            analysis_task = Task(
                description=f"""Analyze task result and determine next actions:
                Result: {result}
                
                1. Evaluate success criteria
                2. Identify optimization opportunities
                3. Update learning system
                4. Propose follow-up tasks if needed""",
                expected_output="Analysis and recommendations",
                agent=crew.manager_agent
            )
            
            # Execute analysis
            await crew.execute_tasks([analysis_task])
            
        except Exception as e:
            logger.error(f"Error processing task result: {str(e)}")

    async def initialize_project(self, payload: dict) -> dict:
        """Initialize a project with the created team data."""
        try:
            # Get active crew
            crew_id = payload.get("crew_id")
            if not crew_id or crew_id not in self.active_crews:
                raise ValueError("Invalid crew ID")
            
            crew = self.active_crews[crew_id]
            
            # Create initialization tasks
            init_tasks = [
                Task(
                    description=f"""Initialize project environment:
                    Project: {payload.get('description')}
                    
                    1. Set up project structure
                    2. Configure development environment
                    3. Initialize version control
                    4. Set up CI/CD pipelines
                    5. Configure monitoring and logging""",
                    expected_output="Project initialization report",
                    agent=crew.manager_agent
                ),
                Task(
                    description="""Create project management structure:
                    1. Define milestones
                    2. Create task breakdown
                    3. Set up tracking metrics
                    4. Configure collaboration tools
                    5. Establish review processes""",
                    expected_output="Project management setup report",
                    agent=crew.manager_agent
                )
            ]
            
            # Execute initialization
            results = await crew.execute_tasks(init_tasks)
            
            return {
                "crew_id": crew_id,
                "initialization_results": results,
                "status": "initialized"
            }
            
        except Exception as e:
            logger.error(f"Error initializing project: {str(e)}")
            return {"error": str(e)}

    async def create_agent(self, spec: dict) -> dict:
        """Create a new agent with specified capabilities."""
        try:
            # Create agent with enhanced capabilities
            agent = DynamicAgent(
                role=spec.get("role"),
                goal=spec.get("goal"),
                backstory=spec.get("backstory"),
                verbose=True,
                allow_delegation=True,
                tools=self.default_tools.copy()
            )
            
            # Configure agent
            await agent.configure_collaboration(spec.get("collaboration", {}))
            await agent.setup_learning_system(spec.get("learning", {}))
            await agent.set_autonomy_level(spec.get("autonomy_level", 0.5))
            
            # Store agent
            self.active_agents[agent.id] = agent
            
            return {
                "id": agent.id,
                "role": agent.role,
                "status": "created",
                "capabilities": {
                    "tools": [tool.name for tool in agent.tools],
                    "autonomy_level": agent.autonomy_level,
                    "skills": agent.skills
                }
            }
            
        except Exception as e:
            logger.error(f"Error creating agent: {str(e)}")
            return {"error": str(e)}

    def get_agents(self) -> list:
        """Get all active agents."""
        try:
            return [
                {
                    "id": agent.id,
                    "role": agent.role,
                    "status": agent.status,
                    "current_tasks": len([t for t in self.project_manager.tasks.values() 
                                       if agent.id in t.state.assigned_agents]),
                    "capabilities": {
                        "tools": [tool.name for tool in agent.tools],
                        "autonomy_level": agent.autonomy_level,
                        "skills": agent.skills
                    }
                }
                for agent in self.active_agents.values()
            ]
        except Exception as e:
            logger.error(f"Error getting agents: {str(e)}")
            return []

    async def send_crew_message(self, payload: dict) -> dict:
        """Send a message to the entire team and get responses."""
        try:
            message = payload.get("message")
            if not message:
                raise ValueError("Message is required")
            
            responses = []
            
            # Create message handling task for each agent
            for agent_id, agent in self.active_agents.items():
                message_task = Task(
                    description=f"""Process and respond to team message:
                    Message: {message}
                    
                    Consider:
                    1. Message context and intent
                    2. Team-wide implications
                    3. Your role's perspective
                    4. Required actions
                    5. Collaboration opportunities""",
                    expected_output="Processed response",
                    agent=agent
                )
                
                # Execute task
                response = await agent.execute_task(message_task)
                responses.append({
                    "agentId": agent_id,
                    "response": response,
                    "timestamp": datetime.now().isoformat()
                })
            
            return {
                "type": "team_response",
                "responses": responses
            }
            
        except Exception as e:
            logger.error(f"Error sending team message: {str(e)}")
            return {"error": str(e)}

    async def send_agent_message(self, payload: dict) -> dict:
        """Send a message to a specific agent and get response."""
        try:
            agent_id = payload.get("agentId")
            message = payload.get("message")
            
            if not agent_id or agent_id not in self.active_agents:
                raise ValueError("Invalid agent ID")
            if not message:
                raise ValueError("Message is required")
            
            agent = self.active_agents[agent_id]
            
            # Create message handling task
            message_task = Task(
                description=f"""Process and respond to direct message:
                Message: {message}
                
                Consider:
                1. Message context and intent
                2. Required actions
                3. Your specific expertise
                4. Appropriate response format
                5. Follow-up actions needed""",
                expected_output="Processed response",
                agent=agent
            )
            
            # Execute task
            response = await agent.execute_task(message_task)
            
            return {
                "type": "agent_response",
                "agentId": agent_id,
                "response": response,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error sending message: {str(e)}")
            return {"error": str(e)}

    async def analyze_requirements(self, payload: dict) -> list:
        """Analyze requirements and create/update agents."""
        try:
            # Create VP of Engineering for analysis
            vp = DynamicAgent()
            
            # Create analysis task
            analysis_task = Task(
                description=f"""Analyze requirements and propose system updates:
                Requirements: {payload.get('requirements')}
                
                Provide:
                1. Required new agent roles
                2. Updates to existing agents
                3. New tool requirements
                4. Task reorganization needs
                5. Collaboration pattern updates""",
                expected_output="Analysis and recommendations JSON",
                agent=vp
            )
            
            # Execute analysis
            result = await vp.execute_task(analysis_task)
            analysis = json.loads(result)
            
            # Implement recommendations
            updates = []
            for update in analysis.get("updates", []):
                if update["type"] == "new_agent":
                    agent_result = await self.create_agent(update["specification"])
                    updates.append({"type": "agent_created", "result": agent_result})
                elif update["type"] == "agent_update":
                    agent = self.active_agents.get(update["agent_id"])
                    if agent:
                        await agent.update_configuration(update["changes"])
                        updates.append({"type": "agent_updated", "agent_id": agent.id})
                elif update["type"] == "new_tool":
                    tool_result = await self._create_tool(update["specification"])
                    updates.append({"type": "tool_created", "result": tool_result})
            
            return updates
            
        except Exception as e:
            logger.error(f"Error analyzing requirements: {str(e)}")
            return [{"error": str(e)}]

    async def create_task(self, payload: dict) -> dict:
        """Create a new task and assign to appropriate agent."""
        try:
            # Create task specification
            task_spec = {
                "description": payload.get("description"),
                "name": payload.get("name"),
                "required_skills": payload.get("required_skills", []),
                "priority": payload.get("priority", 1),
                "estimated_duration": payload.get("estimated_duration", 1.0),
                "max_parallel_agents": payload.get("max_parallel_agents", 1),
                "min_required_agents": payload.get("min_required_agents", 1),
                "dependencies": payload.get("dependencies", [])
            }
            
            # Find best agent
            assigned_agent = self._find_best_agent_for_task(
                task_spec, 
                list(self.active_agents.values())
            )
            
            if not assigned_agent:
                raise ValueError("No suitable agent found for task")
            
            # Create project task
            task = await self._create_task_from_spec(task_spec, [assigned_agent])
            
            if not task:
                raise ValueError("Failed to create task")
            
            # Add to project manager
            await self.project_manager.add_task(task)
            
            return {
                "id": task.state.id,
                "name": payload.get("name"),
                "status": task.state.status,
                "assigned_to": assigned_agent.id,
                "description": payload.get("description")
            }
            
        except Exception as e:
            logger.error(f"Error creating task: {str(e)}")
            return {"error": str(e)}

    async def _create_tool(self, spec: Dict) -> Dict:
        """Create a new tool based on specification."""
        try:
            # Create tool using DynamicAgent's tool creation capability
            tool = await self.active_agents[spec.get("creator_agent_id")].create_tool(spec)
            
            if tool:
                return {
                    "name": tool.name,
                    "description": tool.description,
                    "status": "created"
                }
            return {"error": "Failed to create tool"}
            
        except Exception as e:
            logger.error(f"Error creating tool: {str(e)}")
            return {"error": str(e)}

================
File: core/__init__.py
================
"""
Tribe core module - Contains core functionality and base classes
"""

================
File: core/crew_collab.py
================
from enum import Enum

class CollaborationMode(Enum):
    """Enum for different collaboration modes between crew members."""
    SEQUENTIAL = "sequential"  # Agents work one after another
    PARALLEL = "parallel"      # Agents work simultaneously
    HYBRID = "hybrid"         # Mix of sequential and parallel work

================
File: core/dynamic_flow_generator.py
================
from crewai.flow.flow import Flow, listen, start
from typing import Dict, List, Any, Optional
import json
import inspect

class DynamicFlowGenerator:
    """Generates dynamic flows based on agent analysis and requirements"""
    
    def __init__(self):
        self.flow_templates = {}
        self.generated_flows = {}
    
    def analyze_and_generate_flow(self, requirements: Dict[str, Any], context: Dict[str, Any]) -> str:
        """Analyzes requirements and generates a new flow class dynamically"""
        
        # Create a flow definition based on requirements
        flow_definition = self._create_flow_definition(requirements, context)
        
        # Generate the flow class dynamically
        flow_class = self._generate_flow_class(flow_definition)
        
        # Register the flow for future use
        flow_id = f"dynamic_flow_{len(self.generated_flows)}"
        self.generated_flows[flow_id] = flow_class
        
        return flow_id
    
    def _create_flow_definition(self, requirements: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """Creates a flow definition based on requirements and context"""
        
        # Analyze requirements to determine needed steps
        steps = []
        dependencies = []
        
        # Determine initial state requirements
        state_requirements = {
            'input_requirements': requirements,
            'context': context,
            'execution_history': []
        }
        
        # Analyze the task to determine necessary steps
        if 'task_type' in requirements:
            if requirements['task_type'] == 'code_modification':
                steps.extend([
                    ('analyze_current_code', {'requires': []}),
                    ('identify_changes', {'requires': ['analyze_current_code']}),
                    ('implement_changes', {'requires': ['identify_changes']}),
                    ('verify_changes', {'requires': ['implement_changes']})
                ])
            elif requirements['task_type'] == 'architecture_design':
                steps.extend([
                    ('analyze_requirements', {'requires': []}),
                    ('design_architecture', {'requires': ['analyze_requirements']}),
                    ('validate_design', {'requires': ['design_architecture']}),
                    ('generate_implementation_plan', {'requires': ['validate_design']})
                ])
        
        # Add any custom steps from requirements
        if 'custom_steps' in requirements:
            for step in requirements['custom_steps']:
                steps.append((step['name'], {'requires': step.get('requires', [])}))
        
        return {
            'steps': steps,
            'dependencies': dependencies,
            'state_requirements': state_requirements
        }
    
    def _generate_flow_class(self, flow_definition: Dict[str, Any]) -> type:
        """Generates a new Flow class dynamically based on the definition"""
        
        class_dict = {
            '__module__': __name__,
            'state_requirements': flow_definition['state_requirements']
        }
        
        # Generate the start method (first step)
        first_step = flow_definition['steps'][0]
        
        def generate_step_method(step_name: str, step_config: Dict[str, Any]):
            def step_method(self, previous_output: Any = None):
                # Record step execution in history
                self.state['execution_history'].append({
                    'step': step_name,
                    'input': previous_output
                })
                
                # Execute the step using appropriate agent
                result = self._execute_step(step_name, previous_output)
                
                # Store result in state
                self.state[f'{step_name}_result'] = result
                return result
            
            # Set the method name
            step_method.__name__ = step_name
            return step_method
        
        # Add the start method
        start_method = generate_step_method(first_step[0], first_step[1])
        class_dict[first_step[0]] = start()(start_method)
        
        # Generate methods for remaining steps
        for i in range(1, len(flow_definition['steps'])):
            step = flow_definition['steps'][i]
            step_method = generate_step_method(step[0], step[1])
            
            # Get the dependencies for this step
            depends_on = step[1]['requires']
            if depends_on:
                # Create a listener that waits for all required steps
                step_method = listen(*depends_on)(step_method)
            else:
                # If no explicit dependencies, listen to the previous step
                step_method = listen(flow_definition['steps'][i-1][0])(step_method)
            
            class_dict[step[0]] = step_method
        
        # Add helper methods
        def _execute_step(self, step_name: str, input_data: Any) -> Any:
            """Executes a single step using appropriate agent"""
            # Get the agent for this step type
            agent = self._get_agent_for_step(step_name)
            
            # Execute the step
            result = agent.execute_task({
                'step_name': step_name,
                'input_data': input_data,
                'context': self.state['context']
            })
            
            return result
        
        def _get_agent_for_step(self, step_name: str) -> Any:
            """Returns the appropriate agent for a given step"""
            # This could be enhanced to use a more sophisticated agent selection mechanism
            from .dynamic import DynamicAgent
            
            return DynamicAgent(
                name=f"{step_name}_agent",
                role=f"Expert in {step_name}",
                goal=f"Successfully execute {step_name}",
                backstory=f"An expert agent specialized in {step_name}",
                allow_delegation=True,
                allow_code_execution=True
            )
        
        class_dict['_execute_step'] = _execute_step
        class_dict['_get_agent_for_step'] = _get_agent_for_step
        
        # Create and return the new Flow class
        return type('DynamicGeneratedFlow', (Flow,), class_dict)
    
    def get_flow(self, flow_id: str) -> Optional[type]:
        """Retrieves a previously generated flow class"""
        return self.generated_flows.get(flow_id)

================
File: core/dynamic.py
================
import os
from typing import Any, Dict, List, Optional, Union, Literal
from pydantic import Field, PrivateAttr, ConfigDict, BaseModel
from crewai import Agent, Task, Process
from ..tools.tool_manager import DynamicToolManager
import threading
import asyncio
import json
import logging
import uuid
from datetime import datetime
from ..tools.system_tools import SystemAccessManager

# Setup logging
logging.basicConfig(level=logging.INFO)

# Core System Configuration Models
class TaskExecutionConfig(BaseModel):
    """Configuration for task execution behavior"""
    allow_parallel: bool = Field(default=True, description="Allow parallel task execution")
    allow_delegation: bool = Field(default=True, description="Allow task delegation between agents")
    max_concurrent_tasks: int = Field(default=10, ge=1, description="Maximum concurrent tasks")
    retry_failed_tasks: bool = Field(default=True, description="Retry failed tasks automatically")
    max_retries: int = Field(default=3, ge=0, description="Maximum retry attempts")
    use_enhanced_scheduling: bool = Field(default=True, description="Use enhanced task scheduling")

class TeamCreationConfig(BaseModel):
    """Configuration for team creation process"""
    max_retries: int = Field(default=3, ge=0, description="Maximum retry attempts for team creation")
    retry_delay: float = Field(default=1.0, ge=0.0, description="Delay between retries in seconds")
    validation_timeout: float = Field(default=30.0, ge=0.0, description="Timeout for team validation in seconds")
    min_agents_required: int = Field(default=1, ge=1, description="Minimum number of agents required")
    max_team_size: int = Field(default=10, ge=1, description="Maximum team size")

class TeamValidationResult(BaseModel):
    """Result of team validation"""
    is_valid: bool = Field(default=False)
    missing_roles: List[str] = Field(default_factory=list)
    missing_tools: List[str] = Field(default_factory=list)
    validation_errors: List[str] = Field(default_factory=list)
    recommendations: List[str] = Field(default_factory=list)

class SystemConfig(BaseModel):
    """Core system configuration"""
    api_endpoint: str = Field(..., description="API endpoint for AI operations")
    collaboration_mode: Literal["SOLO", "HYBRID", "FULL"] = Field(default="HYBRID")
    process_type: Literal["sequential", "hierarchical"] = Field(default="hierarchical")
    task_execution: TaskExecutionConfig = Field(default_factory=TaskExecutionConfig)
    team_creation: TeamCreationConfig = Field(default_factory=TeamCreationConfig)
    debug: bool = Field(default=False)
    max_rpm: int = Field(default=60, ge=1)
    cache_enabled: bool = Field(default=True)

class AgentMetadata(BaseModel):
    """Essential agent metadata for system operations"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    name: str
    role: str
    status: Literal["ready", "busy", "error", "dissolved"] = Field(default="ready")
    current_load: int = Field(default=0, ge=0)
    last_active: Optional[float] = None
    skills: List[str] = Field(default_factory=list)

class TaskMetadata(BaseModel):
    """Essential task metadata for system operations"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    status: Literal["pending", "in_progress", "completed", "failed", "cancelled"] = "pending"
    priority: int = Field(default=0)
    assigned_to: Optional[str] = None
    created_at: float = Field(default_factory=lambda: asyncio.get_event_loop().time())
    started_at: Optional[float] = None
    completed_at: Optional[float] = None
    retries: int = Field(default=0)

class TeamState(BaseModel):
    """Team state management"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    active_agents: List[AgentMetadata] = Field(default_factory=list)
    pending_tasks: List[TaskMetadata] = Field(default_factory=list)
    active_tasks: List[TaskMetadata] = Field(default_factory=list)
    completed_tasks: List[TaskMetadata] = Field(default_factory=list)
    creation_time: float = Field(default_factory=lambda: asyncio.get_event_loop().time())
    last_modified: float = Field(default_factory=lambda: asyncio.get_event_loop().time())
    status: Literal["forming", "active", "dissolving", "dissolved"] = "forming"

class SystemState(BaseModel):
    """Global system state"""
    teams: Dict[str, TeamState] = Field(default_factory=dict)
    available_agents: List[AgentMetadata] = Field(default_factory=list)
    global_tasks: List[TaskMetadata] = Field(default_factory=list)
    system_load: float = Field(default=0.0)
    last_cleanup: Optional[float] = None

# Response Schema Models
class RoleRequirement(BaseModel):
    """Model for role requirements"""
    role: str = Field(..., description="Role name")
    goal: str = Field(..., description="Role's primary objective")
    required_skills: List[str] = Field(..., description="Required skills for the role")
    collaboration_pattern: str = Field(..., description="Preferred collaboration mode")
    min_agents: int = Field(default=1, ge=1)
    max_agents: int = Field(default=3, ge=1)

class TeamStructure(BaseModel):
    """Model for team structure"""
    hierarchy: str = Field(..., description="Team hierarchy type (flat/hierarchical)")
    communication: str = Field(..., description="Communication patterns and protocols")
    coordination: str = Field(..., description="Coordination mechanisms")

class ToolRequirement(BaseModel):
    """Model for tool requirements"""
    name: str = Field(..., description="Tool name")
    purpose: str = Field(..., description="Tool purpose")
    required_by: List[str] = Field(..., description="Roles requiring this tool")

class InitialTask(BaseModel):
    """Model for initial tasks"""
    description: str = Field(..., description="Task description")
    assigned_to: str = Field(..., description="Role name of assignee")
    dependencies: List[str] = Field(default_factory=list)
    expected_output: str = Field(..., description="Expected result")

class TeamCompositionResponse(BaseModel):
    """Model for team composition response"""
    required_roles: List[RoleRequirement] = Field(..., min_items=1)
    team_structure: TeamStructure
    tools_required: List[ToolRequirement] = Field(default_factory=list)
    initial_tasks: List[InitialTask] = Field(default_factory=list)

class OptimizationAction(BaseModel):
    """Model for optimization actions"""
    action: str = Field(..., description="Action type (add/remove/modify)")
    role: str = Field(..., description="Role name")
    reason: str = Field(..., description="Explanation for the action")

class RoleAdjustment(BaseModel):
    """Model for role adjustments"""
    agent: str = Field(..., description="Agent name")
    new_role: str = Field(..., description="New role name")
    reason: str = Field(..., description="Explanation for the adjustment")

class CollaborationUpdate(BaseModel):
    """Model for collaboration updates"""
    pattern: str = Field(..., description="New collaboration pattern")
    affected_roles: List[str] = Field(..., description="Affected roles")
    reason: str = Field(..., description="Explanation for the update")

class ToolRecommendation(BaseModel):
    """Model for tool recommendations"""
    tool: str = Field(..., description="Tool name")
    action: str = Field(..., description="Action (add/remove)")
    for_roles: List[str] = Field(..., description="Target roles")
    reason: str = Field(..., description="Explanation for the recommendation")

class TeamOptimizationResponse(BaseModel):
    """Model for team optimization response"""
    composition_changes: List[OptimizationAction] = Field(default_factory=list)
    role_adjustments: List[RoleAdjustment] = Field(default_factory=list)
    collaboration_updates: List[CollaborationUpdate] = Field(default_factory=list)
    tool_recommendations: List[ToolRecommendation] = Field(default_factory=list)


class ToolConfig(BaseModel):
    """Configuration for a tool"""
    name: str = Field(..., description="Tool name")
    description: str = Field(..., description="Tool description")
    parameters: Dict[str, Any] = Field(default_factory=dict)
    return_type: str = Field(default="Any")
    category: str = Field(default="custom")
    is_dynamic: bool = Field(default=True)


class AgentModel(BaseModel):
    """Model for an agent"""
    name: str = Field(..., description="Unique name of the agent")
    role: str = Field(..., description="Role/title of the agent")
    backstory: str = Field(..., description="Detailed background and expertise")
    goal: str = Field(..., description="Primary objective")
    verbose: bool = Field(default=True)
    allow_delegation: bool = Field(default=True)
    allow_code_execution: bool = Field(default=True)

    model_config = ConfigDict(arbitrary_types_allowed=True)


class TaskModel(BaseModel):
    """Model for a task"""
    description: str = Field(..., description="Detailed task description")
    expected_output: str = Field(..., description="Expected format and content")
    agent: str = Field(..., description="Name of assigned agent")
    tools: List[str] = Field(default_factory=list)
    output_file: Optional[str] = None
    context: Optional[Dict[str, Any]] = Field(default_factory=dict)

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def to_dict(self) -> Dict[str, Any]:
        """Convert task to dictionary format"""
        return {
            "description": self.description,
            "expected_output": self.expected_output,
            "agent": self.agent,
            "tools": self.tools,
            "output_file": self.output_file,
            "context": self.context
        }

    @classmethod
    def from_task(cls, task: Task) -> "TaskModel":
        """Create TaskModel from Task instance"""
        return cls(
            description=task.description,
            expected_output=task.expected_output,
            agent=task.agent,
            tools=task.tools if hasattr(task, "tools") else [],
            output_file=task.output_file if hasattr(task, "output_file") else None,
            context=task.context if hasattr(task, "context") else {}
        )


class ToolModel(BaseModel):
    """Model for a tool"""
    name: str = Field(..., description="Tool name")
    description: str = Field(..., description="Tool description and purpose")
    capabilities: List[str] = Field(..., min_length=1)

    model_config = ConfigDict(arbitrary_types_allowed=True)


class FlowModel(BaseModel):
    """Model for a workflow"""
    name: str = Field(..., description="Flow name")
    description: str = Field(..., description="Flow description")
    steps: List[str] = Field(..., min_length=1)

    model_config = ConfigDict(arbitrary_types_allowed=True)


class TeamResponse(BaseModel):
    """Model for team response"""
    vision: str = Field(..., description="Project vision statement")
    agents: List[AgentModel] = Field(..., min_length=1)
    tasks: List[TaskModel] = Field(..., min_length=1)
    tools: List[ToolModel] = Field(..., min_length=1)
    flows: List[FlowModel] = Field(..., min_length=1)

    model_config = ConfigDict(extra="forbid")


class AgentState(BaseModel):
    """Structured state data for DynamicAgent"""

    name: str = Field(default="")
    created_agents: List[Agent] = Field(default_factory=list)
    collaboration_mode: str = Field(default="HYBRID")
    current_team: Optional[str] = Field(default=None)
    status: str = Field(default="ready")
    api_endpoint: str = Field(default="")
    tools_list: List[Any] = Field(default_factory=list)

    model_config = ConfigDict(arbitrary_types_allowed=True, extra="allow")

    def to_dict(self) -> Dict[str, Any]:
        """Convert state to dictionary"""
        return {
            "name": self.name,
            "created_agents": self.created_agents,
            "collaboration_mode": self.collaboration_mode,
            "current_team": self.current_team,
            "status": self.status,
            "api_endpoint": self.api_endpoint,
            "tools": self.tools_list
        }


class VPEngineeringModel(TeamResponse):
    """Model for VP of Engineering initialization, inheriting from TeamResponse"""
    model_config = ConfigDict(arbitrary_types_allowed=True)


class DynamicAgent(Agent):
    """Specialized agent for bootstrapping teams and analyzing project requirements"""
    
    # Define the system_manager field as a private attribute
    _system_manager: Optional[SystemAccessManager] = PrivateAttr(default=None)
    
    def __init__(self, role: str = "VP of Engineering", goal: str = None, backstory: str = None, api_endpoint: str = None):
        """Initialize DynamicAgent with API endpoint"""
        super().__init__(
            role=role,
            goal=goal or "Create and evolve optimal agent teams for projects",
            backstory=backstory or """You are responsible for analyzing project requirements
            and creating optimal teams of AI agents. You understand different roles, skills,
            and collaboration patterns needed for successful project execution.""",
            tools=[],  # Initialize with empty tools list
            api_endpoint=api_endpoint,
            verbose=True,
            allow_delegation=True
        )
        
        # Initialize cache and state
        self._analysis_cache = {}
        self._state = {
            "status": "ready",
            "api_endpoint": api_endpoint,
            "initialization_complete": False,
            "project_context": {},
            "role_context": {
                "role": role,
                "capabilities": [],
                "knowledge_domain": [],
                "interaction_history": [],
                "self_awareness": {
                    "can_delegate": True,
                    "authority_level": "high" if role == "VP of Engineering" else "standard",
                    "can_modify_team": role == "VP of Engineering"
                }
            }
        }
        
        # Initialize system access tools using private attribute
        self._system_manager = SystemAccessManager()
        self.tools.extend(self._system_manager.get_tools())

    def get_role_context(self) -> dict:
        """Get the current role context for self-referential operations"""
        return self._state["role_context"]

    def update_role_context(self, **kwargs):
        """Update role-specific context"""
        self._state["role_context"].update(kwargs)

    async def verify_system_access(self, system_name: str) -> dict:
        """Verify access to a specific system using the appropriate tool."""
        if not self._system_manager:
            return {
                "has_access": False,
                "error": "System manager not initialized",
                "last_verified": datetime.now().isoformat()
            }
            
        tool = self._system_manager.get_tool(system_name)
        if not tool:
            return {
                "has_access": False,
                "error": f"System {system_name} not found",
                "last_verified": datetime.now().isoformat()
            }
        
        return await tool.execute(agent_role=self.role)

    async def handle_self_referential_query(self, query: str) -> str:
        """Handle queries about the agent's own role and capabilities"""
        context = self.get_role_context()
        
        # Check if query is about system access
        if any(keyword in query.lower() for keyword in ["access", "system", "learning", "project management"]):
            # Verify access for all systems using tools
            access_status = {}
            for system in ["learning_system", "project_management", "collaboration_tools"]:
                access_status[system] = await self.verify_system_access(system)
            
            context["system_access_status"] = access_status
        
        return await self.execute_task({
            "description": f"Answer the following self-referential query based on your role context:\n{query}",
            "context": context,
            "expected_output": "Contextually aware response based on role and capabilities"
        })

    def update_state(self, **kwargs):
        """Update agent state"""
        self._state.update(kwargs)
        if "project_context" in kwargs:
            self._state["project_context"].update(kwargs["project_context"])

    @property
    def status(self):
        """Get agent status"""
        return self._state["status"]

    @property
    def initialization_complete(self):
        """Get initialization status"""
        return self._state["initialization_complete"]

    @classmethod
    async def create_vp_engineering(cls, project_description: str) -> "DynamicAgent":
        """Create the VP of Engineering that will bootstrap the system"""
        logging.info(f"Creating VP of Engineering for project: {project_description}")
        try:
            # Get API endpoint from environment
            api_endpoint = os.environ.get('AI_API_ENDPOINT')
            if not api_endpoint:
                api_endpoint = "https://teqheaidyjmkjwkvkde65rfmo40epndv.lambda-url.eu-west-3.on.aws/"
                logging.info(f"Using default API endpoint: {api_endpoint}")
            else:
                logging.info(f"Using API endpoint from environment: {api_endpoint}")

            # Initialize VP of Engineering with all available tools
            vp = cls(
                role="VP of Engineering",
                goal=f"Create and evolve an optimal agent ecosystem to build and maintain {project_description}, leveraging parallel execution and built-in systems",
                backstory=f"""You are the VP of Engineering responsible for bootstrapping
                the AI ecosystem. Your purpose is to analyze requirements and create
                the first set of agents needed for building and maintaining a {project_description}.
                
                Available Systems and Tools:
                1. Project Management System:
                   - Task tracking and assignment
                   - Progress monitoring
                   - Resource allocation
                   - Performance metrics
                   - Collaboration tracking
                
                2. Learning System:
                   - Knowledge sharing between agents
                   - Performance improvement tracking
                   - Skill development
                   - Experience accumulation
                   - Adaptation mechanisms
                
                3. Parallel Execution Capabilities:
                   - Multiple instances of the same agent type can be created for parallel task execution
                   - Asynchronous task processing
                   - Concurrent operations
                   - Load balancing between agent instances
                   - Task prioritization and scheduling
                
                You must:
                1. Analyze the project requirements thoroughly
                2. Design a team of specialized AI agents with complementary skills
                   - Create multiple instances of agent types when parallel execution would be beneficial
                   - Configure each agent with access to the project management and learning systems
                3. Define clear roles, responsibilities, and collaboration patterns
                4. Create an initial set of tasks and workflows
                5. Establish communication protocols between agents
                6. Set up the project management system
                7. Configure the learning system for all agents
                8. Ensure the system is self-sustaining
                
                When creating new agents, always:
                1. Grant them access to the project management system
                2. Enable their learning capabilities
                3. Configure their collaboration patterns
                4. Set appropriate autonomy levels
                5. Define their parallel execution preferences
                
                The team you create should be capable of building and maintaining the project
                while adapting to new requirements and challenges. Multiple instances of the same
                agent type can be created to handle parallel workloads efficiently.""",
                api_endpoint=api_endpoint
            )

            # Initialize state with better tracking
            vp.update_state(
                status="active",
                initialization_complete=True,
                project_context={
                    "description": project_description,
                    "initialization_time": asyncio.get_event_loop().time(),
                    "initialization_status": "completed",
                    "role": "VP of Engineering"
                }
            )
            
            logging.info("VP of Engineering created successfully")
            return vp
            
        except Exception as e:
            logging.error(f"Error creating VP of Engineering: {str(e)}", exc_info=True)
            raise

    async def analyze_project(self, project_description: str) -> Dict[str, Any]:
        """Analyze project requirements and determine optimal team structure"""
        try:
            logging.info(f"Analyzing project requirements: {project_description}")
            
            # Create analysis task
            analysis_task = Task(
                description=f"""Analyze project requirements and create team blueprint:
                Project: {project_description}
                
                Create a comprehensive analysis including:
                1. Required agent roles and expertise
                2. Team structure and hierarchy
                3. Communication protocols
                4. Required tools and capabilities
                5. Initial task breakdown
                6. Success metrics
                
                Important: For each agent role, provide a unique and descriptive name that reflects their function, along with a detailed description of their responsibilities and personality traits.""",
                expected_output="""Team blueprint in JSON format:
                {
                    "required_roles": [
                        {
                            "role": "Role name",
                            "name": "Unique descriptive name for the agent",
                            "description": "Detailed description of the agent's responsibilities and personality traits",
                            "goal": "Role's primary objective",
                            "required_skills": ["skill1", "skill2"],
                            "collaboration_pattern": "preferred collaboration mode",
                            "min_agents": 1,
                            "max_agents": 3
                        }
                    ],
                    "team_structure": {
                        "hierarchy": "flat/hierarchical",
                        "communication": "patterns and protocols",
                        "coordination": "coordination mechanisms"
                    },
                    "tools_required": [
                        {
                            "name": "tool name",
                            "purpose": "tool purpose",
                            "required_by": ["role1", "role2"]
                        }
                    ],
                    "initial_tasks": [
                        {
                            "description": "task description",
                            "assigned_to": "role name",
                            "dependencies": ["task1", "task2"],
                            "expected_output": "expected result"
                        }
                    ]
                }""",
                agent=self
            )
            
            # Execute analysis
            result = await self.execute_task(analysis_task)
            
            try:
                blueprint = json.loads(result)
                self._analysis_cache[project_description] = blueprint
                return blueprint
            except json.JSONDecodeError:
                raise ValueError("Invalid analysis result format")
            
        except Exception as e:
            logging.error(f"Error analyzing project: {str(e)}")
            raise
    
    async def create_agent_specs(self, blueprint: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Create agent specifications from project blueprint"""
        try:
            specs = []
            
            for role in blueprint.get("required_roles", []):
                # Create agent specification
                spec = {
                    "name": role.get("name", f"{role['role']} Agent"),  # Use the unique name if provided
                    "role": role["role"],
                    "goal": role["goal"],
                    "backstory": role.get("description", f"""Expert in {role['role']} with skills in {', '.join(role['required_skills'])}.
                    Works best in {role['collaboration_pattern']} collaboration mode."""),
                    "tools": [],  # Tools will be configured by DynamicCrew
                    "collaboration_mode": role["collaboration_pattern"],
                    "min_agents": role.get("min_agents", 1),
                    "max_agents": role.get("max_agents", 1)
                }
                specs.append(spec)
            
            return specs
            
        except Exception as e:
            logging.error(f"Error creating agent specs: {str(e)}")
            raise
    
    async def validate_team(self, team: Dict[str, Any], blueprint: Dict[str, Any]) -> bool:
        """Validate if a team matches project requirements"""
        try:
            # Check if all required roles are filled
            required_roles = {role["role"] for role in blueprint.get("required_roles", [])}
            team_roles = {agent["role"] for agent in team.get("agents", [])}
            
            if not required_roles.issubset(team_roles):
                logging.warning(f"Missing roles: {required_roles - team_roles}")
                return False
            
            # Check if team structure matches requirements
            team_structure = blueprint.get("team_structure", {})
            if team.get("hierarchy") != team_structure.get("hierarchy"):
                logging.warning("Team hierarchy mismatch")
                return False
            
            # Validate tool availability
            required_tools = {tool["name"] for tool in blueprint.get("tools_required", [])}
            available_tools = set()
            for agent in team.get("agents", []):
                available_tools.update(tool["name"] for tool in agent.get("tools", []))
            
            if not required_tools.issubset(available_tools):
                logging.warning(f"Missing tools: {required_tools - available_tools}")
                return False
            
            return True
            
        except Exception as e:
            logging.error(f"Error validating team: {str(e)}")
            return False
    
    async def optimize_team(self, team: Dict[str, Any], performance_metrics: Dict[str, Any]) -> TeamOptimizationResponse:
        """Optimize team composition based on performance metrics"""
        try:
            # Create optimization task
            optimization_task = Task(
                description=f"""Analyze team performance and suggest optimizations:
                Current Team: {json.dumps(team)}
                Performance Metrics: {json.dumps(performance_metrics)}
                
                Suggest improvements for:
                1. Team composition
                2. Role assignments
                3. Collaboration patterns
                4. Tool utilization""",
                expected_output="Optimization suggestions in TeamOptimizationResponse format",
                agent=self
            )
            
            # Execute optimization
            result = await self.execute_task(optimization_task)
            
            try:
                if isinstance(result, str):
                    result = json.loads(result)
                return TeamOptimizationResponse(**result)
            except json.JSONDecodeError:
                raise ValueError("Invalid optimization result format")
            
        except Exception as e:
            logging.error(f"Error optimizing team: {str(e)}")
            # Return a minimal valid response
            return TeamOptimizationResponse(
                composition_changes=[],
                role_adjustments=[],
                collaboration_updates=[],
                tool_recommendations=[]
            )


class DynamicCrew:
    """Dynamic crew management system for agent teams"""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize DynamicCrew with configuration"""
        # Parse and validate configuration
        self.config = config
        self.system_state = {}
        
        # Initialize managers
        self.project_manager = config.get('project_manager')
        
        # Initialize agent pool and teams
        self._agent_pool = []
        self._teams = {}
        
        # Initialize task queue
        self._task_queue = asyncio.Queue()
        
        # Setup logging
        if config.get('debug', False):
            logging.basicConfig(level=logging.DEBUG)

    def get_active_agents(self):
        """Get list of currently active agents"""
        return self._agent_pool

    def add_agent(self, agent: DynamicAgent) -> None:
        """Add an agent to the available pool"""
        if agent not in self._agent_pool:
            self._agent_pool.append(agent)
            logging.info(f"Added agent {agent.name if hasattr(agent, 'name') else agent.role} to pool")

    def remove_agent(self, agent: DynamicAgent) -> None:
        """Remove an agent from the available pool"""
        if agent in self._agent_pool:
            self._agent_pool.remove(agent)
            logging.info(f"Removed agent {agent.name if hasattr(agent, 'name') else agent.role} from pool")

    async def create_team(self, project_description: str) -> Dict[str, Any]:
        """Create a new team based on project requirements"""
        try:
            logging.info(f"Creating team for project: {project_description}")
            
            # Create team ID
            team_id = str(uuid.uuid4())
            
            # Use current agents as team
            selected_agents = self._agent_pool.copy()
            
            # Create team structure
            team = {
                "id": team_id,
                "description": project_description,
                "agents": selected_agents,
                "created_at": asyncio.get_event_loop().time(),
                "status": "active",
                "tasks": []
            }
            
            # Store team
            self._teams[team_id] = team
            
            return {
                "team": {
                    "id": team_id,
                    "description": project_description,
                    "agents": [
                        {
                            "id": str(uuid.uuid4()),
                            "role": agent.role,
                            "status": "active"
                        }
                        for agent in selected_agents
                    ]
                }
            }
            
        except Exception as e:
            logging.error(f"Error creating team: {str(e)}")
            raise

    def _update_agent_metadata(self, agent: DynamicAgent) -> AgentMetadata:
        """Update agent metadata"""
        return AgentMetadata(
            id=str(uuid.uuid4()) if not hasattr(agent, 'id') else agent.id,
            name=agent.name,
            role=agent.role,
            status=agent.status,
            current_load=len(agent._collaboration_tasks),
            last_active=asyncio.get_event_loop().time(),
            skills=[tool.name for tool in agent.tools if hasattr(tool, 'name')]
        )

    def _update_task_metadata(self, task: Task) -> TaskMetadata:
        """Update task metadata"""
        return TaskMetadata(
            id=task.id if hasattr(task, 'id') else str(uuid.uuid4()),
            status="pending",
            priority=task.priority if hasattr(task, 'priority') else 0,
            assigned_to=task.agent.name if hasattr(task, 'agent') else None
        )

    def _update_team_state(self, team_id: str) -> None:
        """Update team state"""
        if team_id in self.system_state.teams:
            team_state = self.system_state.teams[team_id]
            team_state.last_modified = asyncio.get_event_loop().time()
            
            # Update agent states
            for agent in team_state.active_agents:
                agent_obj = next((a for a in self._agent_pool if a.name == agent.name), None)
                if agent_obj:
                    updated_metadata = self._update_agent_metadata(agent_obj)
                    agent.status = updated_metadata.status
                    agent.current_load = updated_metadata.current_load
                    agent.last_active = updated_metadata.last_active

    async def dissolve_team(self, team_id: str) -> bool:
        """Dissolve a team and release its agents"""
        try:
            if team_id not in self._teams:
                return False
            
            team = self._teams[team_id]
            
            # Stop agent tasks
            for agent in team["agents"]:
                await agent.stop_background_tasks()
                agent.current_team = None
            
            # Remove team
            del self._teams[team_id]
            
            logging.info(f"Team {team_id} dissolved")
            return True
            
        except Exception as e:
            logging.error(f"Error dissolving team: {str(e)}")
            return False

    async def regroup_team(self, team_id: str, new_requirements: str) -> Dict[str, Any]:
        """Regroup an existing team based on new requirements"""
        try:
            if team_id not in self._teams:
                raise ValueError(f"Team {team_id} not found")
            
            # Get current team
            current_team = self._teams[team_id]
            
            # Query new composition
            new_composition = await self._query_team_composition(new_requirements)
            
            # Find agents to remove and add
            current_agents = set(current_team["agents"])
            required_roles = new_composition.get("required_roles", [])
            
            # Select new agents
            new_agents = await self._select_agents(required_roles)
            new_agent_set = set(new_agents)
            
            # Determine changes
            agents_to_remove = current_agents - new_agent_set
            agents_to_add = new_agent_set - current_agents
            
            # Update team
            for agent in agents_to_remove:
                await agent.stop_background_tasks()
                agent.current_team = None
                current_team["agents"].remove(agent)
            
            for agent in agents_to_add:
                agent.current_team = team_id
                await agent.start_background_tasks()
                current_team["agents"].append(agent)
            
            current_team["description"] = new_requirements
            
            logging.info(f"Team {team_id} regrouped: {len(agents_to_remove)} agents removed, {len(agents_to_add)} agents added")
            
            return {
                "team": {
                    "id": team_id,
                    "description": new_requirements,
                    "agents": [
                        {
                            "id": str(uuid.uuid4()),
                            "role": agent.role,
                            "goal": agent.goal,
                            "status": "active"
                        }
                        for agent in current_team["agents"]
                    ]
                }
            }
            
        except Exception as e:
            logging.error(f"Error regrouping team: {str(e)}")
            raise

    async def execute_tasks(self, tasks: List[Task], team_id: Optional[str] = None) -> List[str]:
        """Execute tasks with a specific team or best available agents"""
        try:
            results = []
            
            # Get available agents
            available_agents = self._teams[team_id]["agents"] if team_id else self._agent_pool
            
            for task in tasks:
                # Find best agent for task
                agent = self._find_best_agent(task, available_agents)
                if not agent:
                    raise ValueError(f"No suitable agent found for task: {task.description}")
                
                # Schedule and execute task
                await agent.schedule_task(task)
                result = await agent.execute_task(task)
                results.append(result)
            
            return results
            
        except Exception as e:
            logging.error(f"Error executing tasks: {str(e)}")
            raise

    def _find_best_agent(self, task: Task, available_agents: List[DynamicAgent]) -> Optional[DynamicAgent]:
        """Find the best agent for a task from available agents"""
        best_agent = None
        best_score = -1
        
        for agent in available_agents:
            score = self._calculate_agent_score(agent, task)
            if score > best_score:
                best_score = score
                best_agent = agent
        
        return best_agent

    def _calculate_agent_score(self, agent: DynamicAgent, task: Task) -> float:
        """Calculate how well an agent matches a task"""
        score = 0.0
        
        # Check required skills
        if hasattr(task, 'required_skills'):
            matching_skills = sum(1 for skill in task.required_skills if skill in agent.tools)
            score += matching_skills / len(task.required_skills) if task.required_skills else 0
        
        # Check agent load
        if hasattr(agent, '_collaboration_tasks'):
            load_factor = 1.0 - (len(agent._collaboration_tasks) / 10)  # Assume max 10 tasks
            score += load_factor
        
        # Check agent status
        if agent.status == "ready":
            score += 1.0
        
        return score

    async def cleanup(self):
        """Clean up all crews and release resources"""
        try:
            logging.info("Cleaning up all crews")
            
            # Dissolve all teams
            for team_id in list(self._teams.keys()):
                await self.dissolve_team(team_id)
            
            # Clear agent pool
            self._agent_pool.clear()
            
            # Clear queues
            self._task_queue = asyncio.Queue()
            
            logging.info("Crew cleanup completed")
            
        except Exception as e:
            logging.error(f"Error during crew cleanup: {str(e)}")
            raise

class ProjectTask(Task):
    """Enhanced task model with project management capabilities"""
    
    def __init__(self, description: str, expected_output: str, agent: Agent,
                 priority: int = 1, estimated_duration: float = 1.0,
                 required_skills: List[str] = None, max_parallel_agents: int = 1,
                 min_required_agents: int = 1):
        """Initialize ProjectTask"""
        super().__init__(description=description, expected_output=expected_output, agent=agent)
        
        # Additional project management fields
        self.priority = priority
        self.estimated_duration = estimated_duration
        self.required_skills = required_skills or []
        self.max_parallel_agents = max_parallel_agents
        self.min_required_agents = min_required_agents
        
        # Task state
        self.state = TaskMetadata(
            id=str(uuid.uuid4()),
            status="pending",
            priority=priority,
            assigned_to=agent.id if hasattr(agent, 'id') else None
        )
        
        # Dependencies
        self.dependencies = []
        
    def add_dependency(self, task: 'ProjectTask') -> None:
        """Add a dependency to this task"""
        if task.state.id not in self.dependencies:
            self.dependencies.append(task.state.id)

class ProjectManager:
    """Project management system for task tracking and execution"""
    
    def __init__(self):
        """Initialize ProjectManager"""
        self.tasks = {}
        self.active_tasks = set()
        self.completed_tasks = set()
        self.task_dependencies = {}
        self.task_states = {}

    async def add_task(self, task: ProjectTask) -> None:
        """Add a task to the project manager"""
        self.tasks[task.state.id] = task
        self.task_states[task.state.id] = task.state
        
        # Add task dependencies
        if hasattr(task, 'dependencies'):
            self.task_dependencies[task.state.id] = set(task.dependencies)

    async def update_task_state(self, task_id: str, state: Dict[str, Any]) -> None:
        """Update task state"""
        if task_id in self.task_states:
            self.task_states[task_id].update(state)
            
            # Update task sets based on status
            if state.get('status') == 'completed':
                self.active_tasks.discard(task_id)
                self.completed_tasks.add(task_id)
            elif state.get('status') == 'in_progress':
                self.active_tasks.add(task_id)

    async def get_available_tasks(self) -> List[ProjectTask]:
        """Get tasks that are ready to be executed"""
        available_tasks = []
        
        for task_id, task in self.tasks.items():
            # Skip completed or active tasks
            if task_id in self.completed_tasks or task_id in self.active_tasks:
                continue
                
            # Check if all dependencies are completed
            dependencies = self.task_dependencies.get(task_id, set())
            if all(dep in self.completed_tasks for dep in dependencies):
                available_tasks.append(task)
                
        return available_tasks

    async def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """Get current status of a task"""
        return self.task_states.get(task_id)

    async def cleanup(self) -> None:
        """Clean up completed tasks and update states"""
        # Archive completed tasks
        for task_id in self.completed_tasks:
            if task_id in self.tasks:
                del self.tasks[task_id]
                del self.task_states[task_id]
                if task_id in self.task_dependencies:
                    del self.task_dependencies[task_id]

================
File: core/team_optimizer.py
================
"""Team optimization logic for agent crews."""
from typing import List, Dict, Any
from .crew_collab import CollaborationMode

class TeamOptimizer:
    """Optimizes team composition and collaboration strategies."""
    
    def __init__(self):
        self.collaboration_mode = CollaborationMode.SEQUENTIAL
        
    def optimize_team_composition(self, agents: List[Dict[str, Any]], requirements: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Optimize team composition based on requirements.
        
        Args:
            agents: List of agent specifications
            requirements: Task requirements and constraints
            
        Returns:
            Optimized list of agent specifications
        """
        # For now, return agents as is. Future: implement actual optimization
        return agents
        
    def determine_collaboration_mode(self, task_requirements: Dict[str, Any]) -> CollaborationMode:
        """Determine best collaboration mode based on task requirements.
        
        Args:
            task_requirements: Requirements and constraints for the task
            
        Returns:
            Appropriate CollaborationMode for the task
        """
        # For now, default to sequential. Future: implement smart mode selection
        return CollaborationMode.SEQUENTIAL
        
    def assign_roles(self, agents: List[Dict[str, Any]], task: Dict[str, Any]) -> Dict[str, List[str]]:
        """Assign roles to agents based on task requirements.
        
        Args:
            agents: List of agent specifications
            task: Task specification and requirements
            
        Returns:
            Mapping of roles to agent IDs
        """
        # For now, 1:1 mapping. Future: implement smart role assignment
        return {agent["role"]: [agent["id"]] for agent in agents}

================
File: knowledge/user_preference.txt
================
User name is John Doe.
User is an AI Engineer.
User is interested in AI Agents.
User is based in San Francisco, California.

================
File: src/python/tools/__init__.py
================
"""
Tools for Tribe extension
"""

# Import modules to make them available when importing the package
from .linting import lint_file
from .formatting import format_file

# Define what's available when using "from tribe.src.python.tools import *"
__all__ = [
    'lint_file',
    'format_file',
]

================
File: src/python/tools/formatting.py
================
"""
Formatting functionality for Tribe extension
"""
import re
from typing import List

def format_file(content: str) -> str:
    """Format the given file content
    
    This formatter performs the following operations:
    1. Ensures consistent indentation (4 spaces)
    2. Removes trailing whitespace
    3. Ensures files end with a single newline
    
    Returns:
        The formatted content as a string
    """
    # Split content into lines for processing
    lines = content.splitlines()
    formatted_lines = []
    
    for line in lines:
        # Skip empty lines
        if not line.strip():
            formatted_lines.append("")
            continue
        
        # Remove trailing whitespace
        line = line.rstrip()
        
        # Standardize indentation (convert tabs to 4 spaces)
        indent_level = len(line) - len(line.lstrip())
        if '	' in line[:indent_level]:
            spaces_indent = line[:indent_level].replace('	', '    ')
            line = spaces_indent + line[indent_level:]
        
        formatted_lines.append(line)
    
    # Ensure file ends with a single newline
    formatted_content = '\n'.join(formatted_lines) + '\n'
    
    return formatted_content

================
File: src/python/tools/linting.py
================
"""
Linting functionality for Tribe extension
"""
import re
from typing import List, Dict, Any

def lint_file(content: str) -> List[Dict[str, Any]]:
    """Lint the given file content
    
    Returns a list of diagnostic objects with the following structure:
    {
        "line": int,       # 1-based line number
        "column": int,     # 1-based column number
        "type": str,       # "error", "warning", or "info"
        "message": str,    # The diagnostic message
        "code": str        # A code for the diagnostic
    }
    """
    diagnostics = []
    
    # Split content into lines for analysis
    lines = content.splitlines()
    
    # Check for lines that are too long (> 100 characters)
    for i, line in enumerate(lines):
        if len(line) > 100:
            diagnostics.append({
                "line": i + 1,  # 1-based line number
                "column": 101,  # Position where the line becomes too long
                "type": "warning",
                "message": f"Line too long ({len(line)} > 100 characters)",
                "code": "E501"
            })
    
    # Check for trailing whitespace
    for i, line in enumerate(lines):
        if line and line[-1] == ' ':
            diagnostics.append({
                "line": i + 1,
                "column": len(line),
                "type": "warning",
                "message": "Trailing whitespace",
                "code": "W291"
            })
    
    return diagnostics

================
File: src/python/__init__.py
================
"""
Python source code for Tribe extension
"""

================
File: src/__init__.py
================
"""
Tribe extension source code
"""

================
File: tests/e2e/test_crew_system_e2e.py
================
import pytest
import pytest_asyncio
import os
import time
from typing import Dict, Any
import asyncio

from src.python.core.crew_manager import CrewManager
from src.python.core.agent_project_manager import TaskStatus, TaskPriority
from src.python.tools.dynamic_flow_analyzer import DynamicFlowAnalyzer

@pytest_asyncio.fixture
async def crew_manager(tmp_path):
    """Create a CrewManager instance with a real workspace and flow analyzer"""
    workspace_path = str(tmp_path / "test_workspace")
    
    # Create test directories and sample files
    os.makedirs(workspace_path, exist_ok=True)
    os.makedirs(os.path.join(workspace_path, 'agents'), exist_ok=True)
    os.makedirs(os.path.join(workspace_path, 'flows'), exist_ok=True)
    os.makedirs(os.path.join(workspace_path, 'projects'), exist_ok=True)
    
    # Create a sample Python file to analyze
    sample_code_dir = os.path.join(workspace_path, 'src')
    os.makedirs(sample_code_dir, exist_ok=True)
    
    with open(os.path.join(sample_code_dir, 'calculator.py'), 'w') as f:
        f.write('''
class Calculator:
    def add(self, a: float, b: float) -> float:
        return a + b
    
    def subtract(self, a: float, b: float) -> float:
        return a - b
    
    def multiply(self, a: float, b: float) -> float:
        return a * b
    
    def divide(self, a: float, b: float) -> float:
        if b == 0:
            raise ValueError("Cannot divide by zero")
        return a / b
''')
    
    manager = CrewManager(workspace_path)
    manager.initialize()
    
    # Create a default project
    project_id = manager.project_manager.create_project({
        'name': 'Test Project',
        'description': 'Project for testing'
    })
    
    # Mock FlowManager methods
    def mock_create_flow(requirements, context):
        flow_id = 'test_flow_' + str(len(manager.flow_manager.active_flows))
        task_id = 'test_task_' + str(len(manager.project_manager.get_tasks(project_id)))
        
        # Create task in project
        task_spec = {
            'name': requirements.get('description', 'Test Task'),
            'description': requirements.get('description', ''),
            'priority': TaskPriority.MEDIUM.value,
            'status': TaskStatus.PENDING.value
        }
        task_id = manager.project_manager.add_task(project_id, task_spec)
        
        # Store flow with context
        flow = {
            'requirements': requirements,
            'context': {
                'project_id': project_id,
                'task_id': task_id
            },
            'status': 'pending'
        }
        manager.flow_manager.active_flows[flow_id] = flow
        return flow_id
    
    def mock_get_flow(flow_id):
        return manager.flow_manager.active_flows.get(flow_id)
    
    async def mock_execute_flow(flow_id):
        flow = manager.flow_manager.active_flows[flow_id]
        flow['status'] = 'completed'
        
        # Create test file if this is a test implementation flow
        if flow['requirements'].get('task_type') == 'implement_test':
            test_file = os.path.join(workspace_path, 'src', 'test_calculator.py')
            with open(test_file, 'w') as f:
                f.write('''
import pytest
from calculator import Calculator

def test_add():
    calc = Calculator()
    assert calc.add(2, 3) == 5

def test_subtract():
    calc = Calculator()
    assert calc.subtract(5, 3) == 2

def test_multiply():
    calc = Calculator()
    assert calc.multiply(4, 3) == 12

def test_divide():
    calc = Calculator()
    assert calc.divide(6, 2) == 3
    with pytest.raises(ValueError):
        calc.divide(1, 0)
''')
        
        return {
            'status': 'completed',
            'result': 'Flow executed successfully'
        }
    
    manager.flow_manager.create_flow = mock_create_flow
    manager.flow_manager.get_flow = mock_get_flow
    manager.flow_manager.execute_flow = mock_execute_flow
    return manager

@pytest.mark.asyncio
async def test_end_to_end_flow(crew_manager):
    """Test a complete end-to-end flow with real code analysis and execution"""
    
    # 1. Create an agent with testing expertise
    agent_spec = {
        "name": "TestExpert",
        "role": "QA Engineer",
        "goal": "Create comprehensive test suites",
        "expertise": ["Python", "Testing", "Test Automation"],
        "backstory": "Senior QA engineer with 10 years of experience in test automation and quality assurance. Expert in writing comprehensive test suites and ensuring code quality."
    }
    
    # 2. Define a flow for implementing calculator tests
    flow_requirements = {
        "task_type": "implement_test",
        "description": "Create unit tests for the Calculator class",
        "language": "python",
        "file_type": "test",
        "estimated_duration": "1h",
        "success_factors": [
            "Test all calculator operations",
            "Include edge cases",
            "Test error handling"
        ],
        "target_file": "calculator.py",
        "test_framework": "pytest"
    }
    
    # 3. Create collaboration
    collab = await crew_manager.create_agent_flow_collaboration(agent_spec, flow_requirements)
    assert collab["agent"]["name"] == "TestExpert"
    
    # 4. Execute the flow with real code analysis
    result = await crew_manager.execute_flow(collab["flow_id"])
    
    # 5. Verify flow execution result
    assert result is not None
    assert "status" in result
    
    # 6. Verify task completion
    task = crew_manager.project_manager.get_task(collab["project_id"], collab["task_id"])
    assert task["status"] in [TaskStatus.COMPLETED.value, TaskStatus.FAILED.value]
    
    # 7. Verify generated test file exists (if flow was successful)
    if task["status"] == TaskStatus.COMPLETED.value:
        test_file = os.path.join(crew_manager.workspace_path, 'src', 'test_calculator.py')
        assert os.path.exists(test_file)

@pytest.mark.asyncio
async def test_real_flow_error_recovery(crew_manager):
    """Test error recovery with real flow execution"""
    
    # 1. Create an agent
    agent_spec = {
        "name": "ErrorHandler",
        "role": "System Tester",
        "goal": "Test system error handling",
        "expertise": ["Error Handling", "Testing"],
        "backstory": "Expert in testing error scenarios"
    }
    
    # 2. Define a flow with invalid requirements
    flow_requirements = {
        "task_type": "implement_test",
        "description": "Test invalid file",
        "language": "python",
        "file_type": "test",
        "target_file": "nonexistent.py"  # This file doesn't exist
    }
    
    # 3. Create collaboration
    collab = await crew_manager.create_agent_flow_collaboration(agent_spec, flow_requirements)
    
    # 4. Execute flow and expect failure
    try:
        result = await crew_manager.execute_flow(collab["flow_id"])
    except Exception as e:
        # Verify task was marked as failed
        task = crew_manager.project_manager.get_task(collab["project_id"], collab["task_id"])
        assert task["status"] == TaskStatus.FAILED.value
        
        # Verify error was recorded in flow history
        flow_status = crew_manager.flow_manager.get_flow_status(collab["flow_id"])
        assert flow_status["status"] == "failed"
        assert "error" in flow_status

@pytest.mark.asyncio
async def test_real_concurrent_flows(crew_manager):
    """Test concurrent flow execution with real flows"""
    
    # Create multiple agents and flows
    agents = [
        {
            "name": f"Agent{i}",
            "role": "Tester",
            "goal": "Execute concurrent tests",
            "expertise": ["Testing"],
            "backstory": f"Specialized test agent {i} with expertise in concurrent execution testing and performance analysis. Focused on identifying race conditions and timing issues."
        }
        for i in range(3)
    ]
    
    flow_requirements = {
        "task_type": "implement_test",
        "description": "Create calculator tests",
        "language": "python",
        "file_type": "test",
        "target_file": "calculator.py"
    }
    
    # Create collaborations
    collabs = []
    for agent_spec in agents:
        collab = await crew_manager.create_agent_flow_collaboration(agent_spec, flow_requirements)
        collabs.append(collab)
    
    # Execute flows concurrently
    async def execute_and_verify(collab):
        try:
            result = await crew_manager.execute_flow(collab["flow_id"])
            return result
        except Exception:
            return None
    
    results = await asyncio.gather(*[
        execute_and_verify(collab)
        for collab in collabs
    ])
    
    # Verify results
    for i, result in enumerate(results):
        task = crew_manager.project_manager.get_task(
            collabs[i]["project_id"],
            collabs[i]["task_id"]
        )
        assert task["status"] in [TaskStatus.COMPLETED.value, TaskStatus.FAILED.value]

@pytest.mark.asyncio
async def test_flow_with_dependencies(crew_manager):
    """Test flow execution with dependent tasks"""
    
    # 1. Create a setup task
    setup_agent = {
        "name": "SetupAgent",
        "role": "Setup Engineer",
        "goal": "Prepare test environment",
        "expertise": ["Testing", "Setup"],
        "backstory": "Environment setup specialist with extensive experience in configuring test environments, managing dependencies, and ensuring reproducible test conditions."
    }
    
    setup_flow = {
        "task_type": "setup",
        "description": "Prepare calculator test environment",
        "language": "python",
        "file_type": "config"
    }
    
    setup_collab = await crew_manager.create_agent_flow_collaboration(setup_agent, setup_flow)
    
    # 2. Create a dependent test task
    test_agent = {
        "name": "TestAgent",
        "role": "Test Engineer",
        "goal": "Execute tests",
        "expertise": ["Testing"],
        "backstory": "Test execution specialist with deep knowledge of test frameworks and result analysis. Expert in identifying edge cases and potential failure points."
    }
    
    test_flow = {
        "task_type": "implement_test",
        "description": "Create and run calculator tests",
        "language": "python",
        "file_type": "test",
        "dependencies": [setup_collab["task_id"]]
    }
    
    test_collab = await crew_manager.create_agent_flow_collaboration(test_agent, test_flow)
    
    # 3. Execute setup flow
    await crew_manager.execute_flow(setup_collab["flow_id"])
    
    # 4. Execute test flow
    result = await crew_manager.execute_flow(test_collab["flow_id"])
    
    # 5. Verify both tasks completed
    setup_task = crew_manager.project_manager.get_task(
        setup_collab["project_id"],
        setup_collab["task_id"]
    )
    test_task = crew_manager.project_manager.get_task(
        test_collab["project_id"],
        test_collab["task_id"]
    )
    
    assert setup_task["status"] == TaskStatus.COMPLETED.value
    assert test_task["status"] in [TaskStatus.COMPLETED.value, TaskStatus.FAILED.value]

================
File: tests/integration/test_crew_system.py
================
import pytest
import pytest_asyncio
import asyncio
import os
from typing import Dict, Any
import os
import sys
from unittest.mock import MagicMock, patch

from src.python.core.crew_manager import CrewManager
from src.python.core.agent_project_manager import TaskStatus, TaskPriority

@pytest.fixture
def mock_flow_analyzer():
    """Create a mock flow analyzer"""
    mock = MagicMock()
    
    class MockFlow:
        def __init__(self):
            self.state = {}
        
        def start(self):
            return {
                'status': 'completed',
                'result': 'Test flow executed successfully'
            }
    
    def mock_get_flow(flow_id):
        if isinstance(flow_id, str):
            return {
                'id': flow_id,
                'preferred_approach': 'test-driven',
                'success_factors': ['Test coverage for all edge cases'],
                'estimated_duration': '2h'
            }
        return MockFlow
    
    mock.get_flow.side_effect = mock_get_flow
    
    mock.analyze_and_generate_flow.return_value = 'test_flow'
    return mock

@pytest_asyncio.fixture
async def crew_manager(tmp_path, mock_flow_analyzer):
    """Create a CrewManager instance with a temporary workspace"""
    workspace_path = str(tmp_path / "test_workspace")
    
    # Create test directories
    os.makedirs(workspace_path, exist_ok=True)
    os.makedirs(os.path.join(workspace_path, 'agents'), exist_ok=True)
    os.makedirs(os.path.join(workspace_path, 'flows'), exist_ok=True)
    os.makedirs(os.path.join(workspace_path, 'projects'), exist_ok=True)
    
    with patch('src.python.core.flow_manager.DynamicFlowAnalyzer', return_value=mock_flow_analyzer):
        manager = CrewManager(workspace_path)
        manager.initialize()
        return manager

@pytest.mark.asyncio
async def test_full_agent_flow_collaboration(crew_manager):
    """Test the complete workflow of creating and using agents with flows"""
    
    # 1. Create an agent with specific expertise
    agent_spec = {
        "name": "TestDev",
        "role": "Python Developer",
        "goal": "Write high-quality Python code with comprehensive test coverage",
        "expertise": ["Python", "Testing", "API Development"],
        "backstory": "Experienced developer focused on writing clean, testable code"
    }
    
    # 2. Define a flow for implementing a test
    flow_requirements = {
        "task_type": "implement_test",
        "description": "Create a unit test for the user authentication module",
        "language": "python",
        "file_type": "test",
        "estimated_duration": "2h",
        "success_factors": [
            "Test coverage for all edge cases",
            "Clear test descriptions"
        ]
    }
    
    # 3. Create collaboration between agent and flow
    collab = await crew_manager.create_agent_flow_collaboration(agent_spec, flow_requirements)
    
    # 4. Verify collaboration was created successfully
    assert collab["agent"]["name"] == "TestDev"
    assert "project_id" in collab
    assert "task_id" in collab
    
    # 5. Check task was created in project
    task = crew_manager.project_manager.get_task(collab["project_id"], collab["task_id"])
    assert task["name"] == "Execute implement_test"
    assert task["status"] == TaskStatus.PENDING.value
    
    # 6. Execute the flow
    result = await crew_manager.execute_flow(collab["flow_id"])
    assert result["status"] == "completed"
    
    # 7. Verify task status was updated
    task = crew_manager.project_manager.get_task(collab["project_id"], collab["task_id"])
    assert task["status"] == TaskStatus.COMPLETED.value

@pytest.mark.asyncio
async def test_learning_system_integration(crew_manager):
    """Test that the learning system properly influences flow execution"""
    
    # 1. Create multiple collaborations to build learning history
    for i in range(3):
        agent_spec = {
            "name": f"TestDev{i}",
            "role": "Python Developer",
            "goal": "Write high-quality Python code with comprehensive test coverage",
            "expertise": ["Python", "Testing"],
            "backstory": "Experienced Python developer with a focus on testing"
        }
        
        flow_requirements = {
            "task_type": "implement_feature",
            "language": "python",
            "description": f"Implement feature {i}"
        }
        
        await crew_manager.create_agent_flow_collaboration(agent_spec, flow_requirements)
    
    # 2. Create a new collaboration and verify learning system recommendations
    agent_spec = {
        "name": "TestDev4",
        "role": "Python Developer",
        "goal": "Write high-quality Python code with comprehensive test coverage",
        "expertise": ["Python"],
        "backstory": "Experienced Python developer with a focus on quality"
    }
    
    flow_requirements = {
        "task_type": "implement_feature",
        "language": "python",
        "description": "Implement new feature"
    }
    
    collab = await crew_manager.create_agent_flow_collaboration(agent_spec, flow_requirements)
    
    # 3. Verify learning system influenced the flow
    flow_metadata = crew_manager.flow_manager.get_flow_metadata(collab["flow_id"])
    assert "preferred_approach" in flow_metadata
    assert "success_factors" in flow_metadata

@pytest.mark.asyncio
async def test_persistence_and_recovery(crew_manager):
    """Test that system state persists and can be recovered"""
    
    # 1. Create initial state
    agent_spec = {
        "name": "TestDev",
        "role": "Python Developer",
        "goal": "Write high-quality Python code with comprehensive test coverage",
        "backstory": "Experienced Python developer with a focus on testing"
    }
    
    flow_requirements = {
        "task_type": "implement_feature",
        "description": "Test feature"
    }
    
    collab1 = await crew_manager.create_agent_flow_collaboration(agent_spec, flow_requirements)
    
    # 2. Create new manager instance with same workspace
    new_manager = CrewManager(crew_manager.workspace_path)
    new_manager.initialize()
    
    # 3. Verify state was recovered
    agents = new_manager.agent_commands.get_agents()
    agent_names = [agent['name'] for agent in agents]
    assert collab1["agent"]["name"] in agent_names
    assert collab1["flow_id"] in new_manager.flow_manager.get_flows()
    
    # 4. Verify can continue operations
    collab2 = await new_manager.create_agent_flow_collaboration(agent_spec, flow_requirements)
    assert collab2["project_id"] == collab1["project_id"]  # Same project

@pytest.mark.asyncio
async def test_multi_agent_collaboration(crew_manager):
    """Test multiple agents working together on related tasks"""
    
    # 1. Create a project with multiple tasks
    project_id = await crew_manager._ensure_project_exists()
    
    # 2. Create multiple agents
    agents = []
    for role in ["Backend Developer", "Frontend Developer", "QA Engineer"]:
        agent_spec = {
            "name": f"Test{role.replace(' ', '')}",
            "role": role,
            "goal": f"Deliver high-quality {role} work",
            "expertise": [role],
            "backstory": f"Experienced {role} with a focus on quality and collaboration"
        }
        collab = await crew_manager.create_agent_flow_collaboration(
            agent_spec,
            {"task_type": "setup", "description": f"Setup for {role}"}
        )
        agents.append(collab["agent"])
    
    # 3. Create dependent tasks
    backend_task = crew_manager.project_manager.add_task(
        project_id,
        {
            "name": "Implement API",
            "description": "Create REST API endpoints",
            "priority": TaskPriority.HIGH.value
        }
    )
    
    frontend_task = crew_manager.project_manager.add_task(
        project_id,
        {
            "name": "Implement UI",
            "description": "Create user interface",
            "priority": TaskPriority.MEDIUM.value,
            "dependencies": [backend_task]
        }
    )
    
    qa_task = crew_manager.project_manager.add_task(
        project_id,
        {
            "name": "Test Integration",
            "description": "Test API and UI integration",
            "priority": TaskPriority.LOW.value,
            "dependencies": [backend_task, frontend_task]
        }
    )
    
    # 4. Assign tasks to appropriate agents
    crew_manager.project_manager.assign_agent(project_id, backend_task, agents[0]["id"])
    crew_manager.project_manager.assign_agent(project_id, frontend_task, agents[1]["id"])
    crew_manager.project_manager.assign_agent(project_id, qa_task, agents[2]["id"])
    
    # 5. Verify task dependencies are respected
    tasks = crew_manager.project_manager.get_tasks(project_id)
    assert tasks[frontend_task]["dependencies"] == [backend_task]
    assert sorted(tasks[qa_task]["dependencies"]) == sorted([backend_task, frontend_task])

================
File: tests/unit/test_crew_manager.py
================
import pytest
import pytest_asyncio
from unittest.mock import MagicMock, patch
import os
import asyncio
from typing import Dict, Any

from src.python.core.crew_manager import CrewManager
from src.python.core.agent_project_manager import TaskStatus, TaskPriority
from src.python.core.flow_manager import FlowManager

@pytest_asyncio.fixture
async def crew_manager(tmp_path):
    """Create a CrewManager instance with a temporary workspace"""
    workspace_path = str(tmp_path / "test_workspace")
    
    # Create test directories
    os.makedirs(workspace_path, exist_ok=True)
    os.makedirs(os.path.join(workspace_path, 'agents'), exist_ok=True)
    os.makedirs(os.path.join(workspace_path, 'flows'), exist_ok=True)
    os.makedirs(os.path.join(workspace_path, 'projects'), exist_ok=True)
    
    # Create a mock flow manager
    flow_manager = MagicMock(spec=FlowManager)
    flow_manager.active_flows = {}
    
    manager = CrewManager(workspace_path)
    manager.flow_manager = flow_manager
    manager.initialize()
    return manager

@pytest.mark.asyncio
async def test_execute_flow_error_handling(crew_manager):
    """Test that flow execution errors are handled properly"""
    # Setup
    flow_id = "test_flow"
    error_message = "Flow execution failed"
    
    # Mock flow manager to raise an exception
    async def mock_error_flow(_):
        raise Exception(error_message)
    crew_manager.flow_manager.execute_flow = mock_error_flow
    
    # Create a test task
    project_id = crew_manager.project_manager.create_project({
        'name': 'Test Project',
        'description': 'Test project for error handling'
    })
    
    task_spec = {
        'name': 'Test Task',
        'description': 'Test task for error handling',
        'priority': TaskPriority.MEDIUM.value,
        'flow_id': flow_id
    }
    task_id = crew_manager.project_manager.add_task(project_id, task_spec)
    
    # Update flow context
    crew_manager.flow_manager.active_flows[flow_id] = {
        'context': {
            'project_id': project_id,
            'task_id': task_id
        }
    }
    
    # Execute flow and verify error handling
    with pytest.raises(Exception) as exc_info:
        await crew_manager.execute_flow(flow_id)
    assert str(exc_info.value) == error_message
    
    # Verify task status was updated to FAILED
    task = crew_manager.project_manager.get_task(project_id, task_id)
    assert task['status'] == TaskStatus.FAILED.value

@pytest.mark.asyncio
async def test_execute_flow_with_missing_context(crew_manager):
    """Test flow execution with missing or invalid context"""
    # Setup flow without context
    flow_id = "test_flow"
    crew_manager.flow_manager.active_flows[flow_id] = {}
    
    # Mock successful flow execution
    async def mock_success_flow(_):
        return {'status': 'completed'}
    crew_manager.flow_manager.execute_flow = mock_success_flow
    
    # Execute flow and verify it completes without error
    result = await crew_manager.execute_flow(flow_id)
    assert result['status'] == 'completed'

@pytest.mark.asyncio
async def test_task_status_transitions(crew_manager):
    """Test that task status transitions work correctly"""
    # Setup
    flow_id = "test_flow"
    
    # Create a test task
    project_id = crew_manager.project_manager.create_project({
        'name': 'Test Project',
        'description': 'Test project for status transitions'
    })
    
    task_spec = {
        'name': 'Test Task',
        'description': 'Test task for status transitions',
        'priority': TaskPriority.MEDIUM.value,
        'flow_id': flow_id
    }
    task_id = crew_manager.project_manager.add_task(project_id, task_spec)
    
    # Verify initial status is PENDING
    task = crew_manager.project_manager.get_task(project_id, task_id)
    assert task['status'] == TaskStatus.PENDING.value
    
    # Update flow context
    crew_manager.flow_manager.active_flows[flow_id] = {
        'context': {
            'project_id': project_id,
            'task_id': task_id
        }
    }
    
    # Mock flow execution with different statuses
    async def mock_status_flow(_):
        return {'status': 'completed'}
    crew_manager.flow_manager.execute_flow = mock_status_flow
    
    # Execute flow and verify status transitions
    await crew_manager.execute_flow(flow_id)
    task = crew_manager.project_manager.get_task(project_id, task_id)
    assert task['status'] == TaskStatus.COMPLETED.value

@pytest.mark.asyncio
async def test_concurrent_flow_execution(crew_manager):
    """Test handling of concurrent flow executions"""
    # Setup multiple flows
    flow_ids = ["flow1", "flow2", "flow3"]
    tasks = []
    
    # Create a test project
    project_id = crew_manager.project_manager.create_project({
        'name': 'Test Project',
        'description': 'Test project for concurrent execution'
    })
    
    # Create tasks and flows
    for flow_id in flow_ids:
        task_spec = {
            'name': f'Task for {flow_id}',
            'description': 'Test concurrent execution',
            'priority': TaskPriority.MEDIUM.value,
            'flow_id': flow_id
        }
        task_id = crew_manager.project_manager.add_task(project_id, task_spec)
        tasks.append(task_id)
        
        # Setup flow context
        crew_manager.flow_manager.active_flows[flow_id] = {
            'context': {
                'project_id': project_id,
                'task_id': task_id
            }
        }
    
    # Mock flow execution with varying completion times
    async def mock_execute_flow(flow_id):
        return {'status': 'completed'}
    
    crew_manager.flow_manager.execute_flow = mock_execute_flow
    
    # Execute flows concurrently
    import asyncio
    await asyncio.gather(*[crew_manager.execute_flow(flow_id) for flow_id in flow_ids])
    
    # Verify all tasks completed
    for task_id in tasks:
        task = crew_manager.project_manager.get_task(project_id, task_id)
        assert task['status'] == TaskStatus.COMPLETED.value

@pytest.mark.asyncio
async def test_flow_execution_timeout(crew_manager):
    """Test handling of flow execution timeouts"""
    # Setup
    flow_id = "test_flow"
    
    # Create a test task
    project_id = crew_manager.project_manager.create_project({
        'name': 'Test Project',
        'description': 'Test project for timeout handling'
    })
    
    task_spec = {
        'name': 'Test Task',
        'description': 'Test task for timeout handling',
        'priority': TaskPriority.MEDIUM.value,
        'flow_id': flow_id
    }
    task_id = crew_manager.project_manager.add_task(project_id, task_spec)
    
    # Update flow context
    crew_manager.flow_manager.active_flows[flow_id] = {
        'context': {
            'project_id': project_id,
            'task_id': task_id
        }
    }
    
    # Mock flow execution with timeout
    async def mock_timeout_flow(_):
        await asyncio.sleep(0.1)  # Simulate slow execution
        raise asyncio.TimeoutError("Flow execution timed out")
    
    crew_manager.flow_manager.execute_flow = mock_timeout_flow
    
    # Execute flow and verify timeout handling
    with pytest.raises(asyncio.TimeoutError):
        await crew_manager.execute_flow(flow_id)
    
    # Verify task status was updated to FAILED
    task = crew_manager.project_manager.get_task(project_id, task_id)
    assert task['status'] == TaskStatus.FAILED.value

================
File: tests/__init__.py
================


================
File: tests/conftest.py
================
import os
import sys
import pytest
from unittest.mock import MagicMock

# Add the project root to the Python path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# Mock dependencies
sys.modules['src.python.tools.dynamic_flow_analyzer'] = MagicMock()
sys.modules['src.python.tools.code_analyzer'] = MagicMock()
sys.modules['src.python.tools.requirement_analyzer'] = MagicMock()

# Set asyncio fixture scope
def pytest_configure(config):
    config.option.asyncio_mode = "strict"
    config.option.asyncio_default_fixture_loop_scope = "function"

@pytest.fixture
def mock_flow_analyzer():
    return MagicMock()

================
File: tests/requirements-test.txt
================
pytest==8.3.4
pytest-asyncio==0.25.3
pytest-mock==3.14.0
pytest-cov==6.0.0
typing-extensions>=4.8.0
pydantic>=2.0.0
crewai>=0.1.0

================
File: tests/run_tests.py
================
import unittest
import sys
import os

# Add the parent directory to Python path for imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Import test modules
from tests.test_autonomous_crew import TestAutonomousCrew

def run_tests():
    # Create test suite
    suite = unittest.TestLoader().loadTestsFromTestCase(TestAutonomousCrew)
    
    # Run tests with verbosity
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Return 0 if tests passed, 1 if any failed
    return 0 if result.wasSuccessful() else 1

if __name__ == '__main__':
    sys.exit(run_tests())

================
File: tests/test_agent_project_manager.py
================
import pytest
from pathlib import Path
from tribe.src.python.core.agent_project_manager import AgentProjectManager

@pytest.fixture
def project_manager():
    return AgentProjectManager()

@pytest.fixture
def mock_project_structure(tmp_path):
    # Create a mock project structure for testing
    project_root = tmp_path / "test_project"
    project_root.mkdir()
    
    # Create some test files and directories
    (project_root / "src").mkdir()
    (project_root / "src" / "components").mkdir()
    (project_root / "src" / "components" / "Button.tsx").write_text(
        "export const Button = () => <button>Click me</button>"
    )
    (project_root / "package.json").write_text('{"name": "test-project"}')
    return project_root

def test_project_manager_initialization(project_manager):
    assert project_manager is not None

def test_get_project_context(project_manager, mock_project_structure):
    project_manager.set_project_root(str(mock_project_structure))
    context = project_manager.get_project_context()
    
    assert context is not None
    assert "projectRoot" in context
    assert "fileStructure" in context
    assert "dependencies" in context

def test_analyze_project_structure(project_manager, mock_project_structure):
    project_manager.set_project_root(str(mock_project_structure))
    structure = project_manager.analyze_project_structure()
    
    assert structure is not None
    assert "src" in structure
    assert "components" in structure["src"]
    assert "Button.tsx" in structure["src"]["components"]

def test_get_file_content(project_manager, mock_project_structure):
    project_manager.set_project_root(str(mock_project_structure))
    content = project_manager.get_file_content("src/components/Button.tsx")
    
    assert content is not None
    assert "Button" in content
    assert "Click me" in content

def test_get_project_dependencies(project_manager, mock_project_structure):
    project_manager.set_project_root(str(mock_project_structure))
    dependencies = project_manager.get_project_dependencies()
    
    assert dependencies is not None
    assert isinstance(dependencies, dict)

def test_find_related_files(project_manager, mock_project_structure):
    project_manager.set_project_root(str(mock_project_structure))
    related_files = project_manager.find_related_files("src/components/Button.tsx")
    
    assert related_files is not None
    assert isinstance(related_files, list)

def test_analyze_code_patterns(project_manager, mock_project_structure):
    project_manager.set_project_root(str(mock_project_structure))
    patterns = project_manager.analyze_code_patterns("src/components")
    
    assert patterns is not None
    assert "componentPatterns" in patterns
    assert "importPatterns" in patterns

def test_get_project_statistics(project_manager, mock_project_structure):
    project_manager.set_project_root(str(mock_project_structure))
    stats = project_manager.get_project_statistics()
    
    assert stats is not None
    assert "fileCount" in stats
    assert "directoryCount" in stats
    assert "languageStats" in stats

def test_validate_file_changes(project_manager, mock_project_structure):
    project_manager.set_project_root(str(mock_project_structure))
    changes = {
        "filesToModify": [
            {
                "path": "src/components/Button.tsx",
                "content": "export const Button = () => <button>New Text</button>"
            }
        ]
    }
    
    validation = project_manager.validate_file_changes(changes)
    assert validation["isValid"] is True
    assert "conflicts" in validation

def test_apply_file_changes(project_manager, mock_project_structure):
    project_manager.set_project_root(str(mock_project_structure))
    changes = {
        "filesToModify": [
            {
                "path": "src/components/Button.tsx",
                "content": "export const Button = () => <button>New Text</button>"
            }
        ]
    }
    
    result = project_manager.apply_file_changes(changes)
    assert result["success"] is True
    assert result["modifiedFiles"] == ["src/components/Button.tsx"]

def test_get_project_type(project_manager, mock_project_structure):
    project_manager.set_project_root(str(mock_project_structure))
    project_type = project_manager.get_project_type()
    
    assert project_type is not None
    assert "type" in project_type
    assert "framework" in project_type

def test_analyze_dependencies(project_manager, mock_project_structure):
    project_manager.set_project_root(str(mock_project_structure))
    dependency_analysis = project_manager.analyze_dependencies()
    
    assert dependency_analysis is not None
    assert "directDependencies" in dependency_analysis
    assert "devDependencies" in dependency_analysis
    assert "unusedDependencies" in dependency_analysis

def test_get_project_configuration(project_manager, mock_project_structure):
    project_manager.set_project_root(str(mock_project_structure))
    config = project_manager.get_project_configuration()
    
    assert config is not None
    assert "packageJson" in config
    assert "tsConfig" in config

def test_analyze_imports(project_manager, mock_project_structure):
    project_manager.set_project_root(str(mock_project_structure))
    import_analysis = project_manager.analyze_imports("src/components/Button.tsx")
    
    assert import_analysis is not None
    assert "imports" in import_analysis
    assert "exports" in import_analysis

def test_get_file_history(project_manager, mock_project_structure):
    project_manager.set_project_root(str(mock_project_structure))
    history = project_manager.get_file_history("src/components/Button.tsx")
    
    assert history is not None
    assert isinstance(history, list)
    assert all("timestamp" in entry for entry in history)
    assert all("changes" in entry for entry in history)

================
File: tests/test_autonomous_crew.py
================
import os
import requests
import asyncio
import pytest
from unittest.mock import patch, MagicMock
from tribe.src.python.classes.base.dynamic import DynamicCrew, DynamicAgent, GenesisAgent
from tribe.src.python.classes.extended.crew_collab import CollaborationMode
from tribe.src.python.tools.agents import AutonomousCrewManager
from crewai import Task

import pytest_asyncio

@pytest_asyncio.fixture
async def crew():
    manager = AutonomousCrewManager()
    genesis_agent = manager.create_genesis_agent()
    return DynamicCrew(
        config={
            'agents': [genesis_agent],
            'tasks': [],
            'manager_agent': genesis_agent,
            'process': Process.hierarchical,
            'verbose': True,
            'manager_llm': None,  # No need for custom LLM, using Lambda
            'function_calling_llm': None,  # No need for custom LLM, using Lambda
            'language': 'en',
            'language_file': None,
            'memory': None,  # Disable memory to avoid OpenAI dependencies
            'memory_config': None,  # Disable memory config
            'cache': True,
            'embedder': None,  # No need for embedder, using Lambda
            'full_output': False,
            'step_callback': None,
            'task_callback': None,
            'share_crew': False,
            'output_log_file': None,
            'prompt_file': None,
            'planning': True,  # Enable planning for test scenarios
            'planning_llm': None  # No need for custom LLM, using Lambda
        }
    )

class TestAutonomousCrew:

    @pytest.mark.asyncio
    async def test_agent_creation(self, crew):
        """Test dynamic agent creation and team formation"""
        # Create a planner agent
        planner_agent = DynamicAgent(
            role="Task Execution Planner",
            goal="Plan and coordinate task execution",
            backstory="Expert in planning and coordinating software development tasks",
            tools=[],
            collaboration_mode=CollaborationMode.HYBRID
        )
        
        # Test the agent's ability to get responses through OpenRouter Lambda
        response = planner_agent.get_response(
            "What are the key steps to plan a new API endpoint development?"
        )
        
        # Verify planner output
        assert response is not None
        assert isinstance(response, str)
        assert len(response) > 0
        
        # Print response for inspection
        print(f"\nPlanner Agent Response:\n{response}")
        
        # Verify response contains relevant planning concepts
        planning_concepts = ['requirements', 'design', 'implementation', 'testing']
        has_relevant_concepts = any(
            concept.lower() in response.lower() 
            for concept in planning_concepts
        )
        assert has_relevant_concepts, "Response should contain relevant planning concepts"

    @pytest.mark.asyncio
    async def test_collaboration(self, crew):
        """Test agent collaboration capabilities"""
        # Create two agents with different expertise
        backend_agent = DynamicAgent(
            role="Backend Developer",
            goal="Implement API endpoints",
            backstory="Expert in backend development and API design",
            tools=[],
            collaboration_mode=CollaborationMode.HYBRID
        )
        
        tester_agent = DynamicAgent(
            role="QA Engineer",
            goal="Ensure code quality and test coverage",
            backstory="Expert in testing and quality assurance",
            tools=[],
            collaboration_mode=CollaborationMode.HYBRID
        )
        
        crew.add_agent(backend_agent)
        crew.add_agent(tester_agent)
        
        # Create a task requiring collaboration
        task = Task(
            description="Implement and test new API endpoint",
            expected_output="Fully tested API endpoint",
            agent=backend_agent
        )
        
        # Request collaboration
        await backend_agent.request_collaboration(
            task=task,
            required_expertise=["testing"]
        )
        
        # Verify collaboration was established
        assert task.id in backend_agent.collaboration_tasks

    def _has_expertise(self, agent, expertise):
        """Helper to check if agent has specific expertise"""
        # In a real implementation, this would check agent's capabilities
        return expertise.lower() in agent.backstory.lower()

    @pytest.mark.asyncio
    async def test_openrouter_integration(self, crew):
        """Test OpenRouter Lambda integration for agent responses"""
        # Create an agent with OpenRouter capabilities
        agent = DynamicAgent(
            role="AI Researcher",
            goal="Provide accurate responses using OpenRouter",
            backstory="Expert in AI research and natural language processing",
            tools=[],
            collaboration_mode=CollaborationMode.HYBRID
        )
        
        test_prompt = "Explain what makes a good API design in 2-3 sentences."
        
        # Test the agent's ability to get responses through OpenRouter Lambda
        response = await agent.execute(Task(
            description=test_prompt,
            expected_output="A concise explanation of good API design principles"
        ))
        
        # Verify the response
        assert response is not None
        print(f"\nAI Response to API design question:\n{response}")
        
        # Verify response contains relevant API design concepts
        api_concepts = ['REST', 'endpoint', 'interface', 'design', 'API']
        has_relevant_concepts = any(concept.lower() in response.lower() for concept in api_concepts)
        assert has_relevant_concepts, "Response should contain relevant API design concepts"

================
File: tests/test_crew_manager.py
================
import pytest
from tribe.src.python.core.crew_manager import CrewManager
from tribe.src.python.core.agent_project_manager import AgentProjectManager

def test_crew_manager_initialization():
    crew_manager = CrewManager()
    assert crew_manager is not None

def test_create_agent():
    crew_manager = CrewManager()
    agent = crew_manager.create_agent("Test Agent", "Developer", "A test agent")
    
    assert agent is not None
    assert agent["name"] == "Test Agent"
    assert agent["role"] == "Developer"
    assert agent["backstory"] == "A test agent"

def test_create_crew():
    crew_manager = CrewManager()
    requirements = "Build a React application"
    crew = crew_manager.create_crew(requirements)
    
    assert crew is not None
    assert len(crew) > 0
    assert all("id" in agent for agent in crew)
    assert all("role" in agent for agent in crew)

def test_agent_interaction():
    crew_manager = CrewManager()
    agent = crew_manager.create_agent("Test Agent", "Developer", "A test agent")
    response = crew_manager.interact_with_agent(agent["id"], "What's your role?")
    
    assert response is not None
    assert isinstance(response, str)
    assert len(response) > 0

def test_crew_task_assignment():
    crew_manager = CrewManager()
    crew = crew_manager.create_crew("Build a React application")
    task = crew_manager.create_task("Create component", "Create a new React component")
    
    assigned_agent = crew_manager.assign_task(task["id"], crew[0]["id"])
    assert assigned_agent is not None
    assert assigned_agent["id"] == crew[0]["id"]

def test_crew_collaboration():
    crew_manager = CrewManager()
    crew = crew_manager.create_crew("Build a React application")
    
    # Test collaboration between agents
    result = crew_manager.collaborate(
        crew[0]["id"],
        crew[1]["id"],
        "Review this code implementation"
    )
    
    assert result is not None
    assert "review" in result
    assert "suggestions" in result

def test_crew_integration_with_project_manager():
    crew_manager = CrewManager()
    project_manager = AgentProjectManager()
    
    # Test crew creation with project context
    project_context = project_manager.get_project_context()
    crew = crew_manager.create_crew("Improve error handling", project_context)
    
    assert crew is not None
    assert len(crew) > 0
    assert all("expertise" in agent for agent in crew)

================
File: tests/test_flow_manager.py
================
import pytest
from tribe.src.python.core.flow_manager import FlowManager
from tribe.src.python.core.agent_project_manager import AgentProjectManager

def test_flow_manager_initialization():
    flow_manager = FlowManager()
    assert flow_manager is not None

def test_flow_generation():
    flow_manager = FlowManager()
    requirements = "Create a new React component"
    flow = flow_manager.generate_flow(requirements)
    
    assert flow is not None
    assert "flowType" in flow
    assert "steps" in flow
    assert "visualizations" in flow

def test_flow_execution():
    flow_manager = FlowManager()
    flow_type = "create_component"
    context = {"componentName": "TestComponent"}
    
    result = flow_manager.execute_flow(flow_type, context)
    assert result is not None
    assert "status" in result
    assert "proposedChanges" in result

def test_flow_visualization():
    flow_manager = FlowManager()
    flow = flow_manager.generate_flow("Create a button component")
    
    assert flow["visualizations"] is not None
    assert len(flow["visualizations"]) > 0
    assert "type" in flow["visualizations"][0]
    assert "content" in flow["visualizations"][0]

def test_flow_proposed_changes():
    flow_manager = FlowManager()
    flow = flow_manager.generate_flow("Create a button component")
    result = flow_manager.execute_flow(flow["flowType"], {})
    
    assert "proposedChanges" in result
    assert "filesToModify" in result["proposedChanges"]
    assert "filesToCreate" in result["proposedChanges"]
    assert "filesToDelete" in result["proposedChanges"]

def test_flow_integration_with_project_manager():
    flow_manager = FlowManager()
    project_manager = AgentProjectManager()
    
    # Test flow generation with project context
    project_context = project_manager.get_project_context()
    flow = flow_manager.generate_flow("Add error handling", project_context)
    
    assert flow is not None
    assert flow["context"]["projectStructure"] is not None

================
File: tests/test_learning_system.py
================
import pytest
from tribe.src.python.core.learning_system import LearningSystem
from tribe.src.python.core.agent_project_manager import AgentProjectManager

def test_learning_system_initialization():
    learning_system = LearningSystem()
    assert learning_system is not None

def test_project_analysis():
    learning_system = LearningSystem()
    project_manager = AgentProjectManager()
    project_context = project_manager.get_project_context()
    
    analysis = learning_system.analyze_project(project_context)
    assert analysis is not None
    assert "patterns" in analysis
    assert "suggestions" in analysis

def test_code_pattern_learning():
    learning_system = LearningSystem()
    code_sample = """
    function MyComponent() {
        return <div>Hello World</div>;
    }
    """
    
    patterns = learning_system.learn_patterns(code_sample)
    assert patterns is not None
    assert len(patterns) > 0
    assert all("type" in pattern for pattern in patterns)

def test_suggestion_generation():
    learning_system = LearningSystem()
    context = {
        "file": "MyComponent.tsx",
        "content": "function MyComponent() { return <div>Hello</div>; }"
    }
    
    suggestions = learning_system.generate_suggestions(context)
    assert suggestions is not None
    assert len(suggestions) > 0
    assert all("description" in suggestion for suggestion in suggestions)

def test_feedback_incorporation():
    learning_system = LearningSystem()
    feedback = {
        "suggestion_id": "123",
        "accepted": True,
        "context": {"file": "test.tsx"}
    }
    
    result = learning_system.incorporate_feedback(feedback)
    assert result is not None
    assert "status" in result
    assert result["status"] == "success"

def test_pattern_matching():
    learning_system = LearningSystem()
    code = "function test() { console.log('hello'); }"
    
    matches = learning_system.match_patterns(code)
    assert matches is not None
    assert isinstance(matches, list)
    assert all("confidence" in match for match in matches)

def test_integration_with_project_manager():
    learning_system = LearningSystem()
    project_manager = AgentProjectManager()
    
    # Test learning from project structure
    project_context = project_manager.get_project_context()
    learning_result = learning_system.learn_from_project(project_context)
    
    assert learning_result is not None
    assert "patterns" in learning_result
    assert "insights" in learning_result

================
File: tools/agents.py
================
from crewai import Agent, Task, Crew, Process
from langchain.tools import Tool
from typing import List, Dict, Optional, Any
import threading
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_agent(specification: str) -> Dict[str, Any]:
    """Create agent with full CrewAI integration"""
    # Return structured data that CrewAI can use
    return {
        "agent_created": True,
        "specification": specification,
        "integration_points": [
            "task_dependencies",
            "hierarchical_process",
            "team_composition"
        ]
    }

class EnhancedCreateAgentTool(Tool):
    """Enhanced tool for AI-driven agent creation with CrewAI integration"""
    def __init__(self):
        super().__init__(
            name="create_agent",
            func=create_agent,
            description="""
            Create a new agent based on your analysis of the current needs and context.
            You can:
            1. Define the agent's role, goal, and backstory naturally
            2. Specify what existing tools they should have access to
            3. Suggest new tools they might need
            4. Define how they should interact with other agents
            5. Specify their position in the hierarchical process
            
            Consider the full context including:
            - Current team composition
            - Task requirements
            - Required expertise
            - Collaboration patterns
            """
        )

def analyze_team(analysis_request: str) -> Dict[str, Any]:
    """Perform team analysis"""
    return {
        "analysis_complete": True,
        "request": analysis_request,
        "suggestions": ["team_adjustments", "new_roles", "process_improvements"]
    }

class TeamAnalysisTool(Tool):
    """Tool for analyzing team composition and suggesting improvements"""
    def __init__(self):
        super().__init__(
            name="analyze_team",
            func=analyze_team,
            description="""
            Analyze the current team composition and suggest improvements.
            You can:
            1. Review current agent capabilities
            2. Identify gaps in expertise
            3. Suggest new agent roles
            4. Recommend structural changes
            5. Analyze task completion patterns
            
            Use this to optimize team performance and structure.
            """
        )

def create_task(task_specification: str) -> Dict[str, Any]:
    """Create or modify tasks"""
    return {
        "task_created": True,
        "specification": task_specification,
        "task_properties": ["dependencies", "context", "criteria"]
    }

class DynamicTaskCreationTool(Tool):
    """Tool for creating and modifying tasks dynamically"""
    def __init__(self):
        super().__init__(
            name="create_task",
            func=create_task,
            description="""
            Create or modify tasks based on emerging needs.
            You can:
            1. Define new tasks with natural language
            2. Specify task dependencies
            3. Assign appropriate agents
            4. Set success criteria
            5. Establish task context
            
            Consider both immediate needs and long-term goals.
            """
        )

class AutonomousCrewManager:
    """Manager class that uses CrewAI's native features for coordination"""
    
    def __init__(self):
        self.tools = [
            EnhancedCreateAgentTool(),
            TeamAnalysisTool(),
            DynamicTaskCreationTool()
        ]
        
    def create_genesis_agent(self) -> Agent:
        """Create the initial genesis agent"""
        return Agent(
            role="Genesis Manager",
            goal="Create and manage an effective agent ecosystem",
            backstory="""You are an expert in team building and management.
            Your purpose is to create and evolve an effective agent ecosystem
            that can handle complex tasks through collaboration.""",
            tools=self.tools,
            allow_delegation=True,
            verbose=True
        )
    
    def initialize_crew(self, objective: str) -> Crew:
        """Initialize a crew with the genesis agent"""
        genesis = self.create_genesis_agent()
        
        # Create initial analysis task
        analysis_task = Task(
            description=f"""
            Analyze the following objective and create an effective agent ecosystem:
            {objective}
            
            1. Determine what types of agents are needed
            2. Create these agents with appropriate roles
            3. Establish their relationships and communication patterns
            4. Create tasks for them to achieve the objective
            """,
            expected_output="""
            A complete description of:
            1. Created agents and their roles
            2. Team structure and relationships
            3. Task breakdown and assignments
            4. Communication and coordination patterns
            """,
            agent=genesis
        )
        
        # Create crew with hierarchical process
        return Crew(
            agents=[genesis],
            tasks=[analysis_task],
            process=Process.hierarchical,  # Enable hierarchical decision making
            planning=True  # Enable AI-driven planning
        )

class DynamicTask(Task):
    """Extended Task class that supports dynamic modification"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.subTasks: List[Task] = []
        self.parentTask: Optional[Task] = None
        
    def add_subtask(self, task: 'DynamicTask'):
        """Add a subtask to this task"""
        task.parentTask = self
        self.subTasks.append(task)
        
    def update_description(self, new_description: str):
        """Update task description based on emerging needs"""
        self.description = new_description

# Example usage showing CrewAI's native features
if __name__ == "__main__":
    # Initialize manager
    manager = AutonomousCrewManager()
    
    # Create crew with an objective
    crew = manager.initialize_crew("""
    We need to build a system that can:
    1. Analyze code performance
    2. Identify bottlenecks
    3. Suggest and implement improvements
    4. Validate the changes
    
    Create whatever agents and tasks you think are necessary.
    """)
    
    # Let the AI-driven system handle everything else
    result = crew.kickoff()
    
    # System is now running with AI-determined structure
    print("Autonomous system operational with AI-driven organization")

================
File: tools/base_tool.py
================
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
from pydantic import BaseModel

class ToolMetadata(BaseModel):
    name: str
    description: str
    parameters: Dict[str, Any]
    return_type: str
    category: str
    created_by: Optional[str] = None
    version: str = "1.0.0"
    is_dynamic: bool = False

class BaseTool(ABC):
    def __init__(self):
        self.metadata = self._get_metadata()

    @abstractmethod
    def _get_metadata(self) -> ToolMetadata:
        """Return the tool's metadata"""
        pass

    @abstractmethod
    async def execute(self, **kwargs) -> Any:
        """Execute the tool with given parameters"""
        pass

    def to_dict(self) -> Dict[str, Any]:
        """Convert tool metadata to dictionary format for LLM consumption"""
        return {
            "name": self.metadata.name,
            "description": self.metadata.description,
            "parameters": self.metadata.parameters,
            "return_type": self.metadata.return_type,
            "category": self.metadata.category,
            "created_by": self.metadata.created_by,
            "version": self.metadata.version,
            "is_dynamic": self.metadata.is_dynamic
        }

================
File: tools/custom_tools.py
================
from typing import Dict, Any, List
from .base_tool import BaseTool, ToolMetadata
from crewai import Agent, Task

class CodeAnalysisTool(BaseTool):
    def _get_metadata(self) -> ToolMetadata:
        return ToolMetadata(
            name="code_analysis",
            description="Analyzes code quality, complexity, and potential issues",
            parameters={
                "code": {
                    "type": "string",
                    "description": "Code to analyze"
                },
                "metrics": {
                    "type": "array",
                    "description": "List of metrics to analyze",
                    "default": ["complexity", "maintainability", "security"]
                }
            },
            return_type="Dict[str, Any]",
            category="code_quality"
        )

    async def execute(self, code: str, metrics: List[str]) -> Dict[str, Any]:
        # Implement code analysis logic
        analysis = {
            "complexity": self._analyze_complexity(code),
            "maintainability": self._analyze_maintainability(code),
            "security": self._analyze_security(code)
        }
        return {metric: analysis[metric] for metric in metrics if metric in analysis}

    def _analyze_complexity(self, code: str) -> Dict[str, Any]:
        # Implement complexity analysis
        return {
            "cyclomatic_complexity": 0,  # Placeholder
            "cognitive_complexity": 0,    # Placeholder
        }

    def _analyze_maintainability(self, code: str) -> Dict[str, Any]:
        # Implement maintainability analysis
        return {
            "maintainability_index": 0,  # Placeholder
            "documentation_ratio": 0,     # Placeholder
        }

    def _analyze_security(self, code: str) -> Dict[str, Any]:
        # Implement security analysis
        return {
            "vulnerabilities": [],        # Placeholder
            "security_score": 0,          # Placeholder
        }

class SystemEvaluationTool(BaseTool):
    def _get_metadata(self) -> ToolMetadata:
        return ToolMetadata(
            name="system_evaluation",
            description="Evaluates system performance, reliability, and functionality",
            parameters={
                "components": {
                    "type": "array",
                    "description": "List of system components to evaluate"
                },
                "metrics": {
                    "type": "array",
                    "description": "Metrics to evaluate",
                    "default": ["performance", "reliability", "functionality"]
                }
            },
            return_type="Dict[str, Any]",
            category="system_analysis"
        )

    async def execute(self, components: List[str], metrics: List[str]) -> Dict[str, Any]:
        evaluation = {}
        for component in components:
            evaluation[component] = {
                "performance": self._evaluate_performance(component),
                "reliability": self._evaluate_reliability(component),
                "functionality": self._evaluate_functionality(component)
            }
        return evaluation

    def _evaluate_performance(self, component: str) -> Dict[str, Any]:
        # Implement performance evaluation
        return {
            "response_time": 0,     # Placeholder
            "throughput": 0,        # Placeholder
            "resource_usage": 0,    # Placeholder
        }

    def _evaluate_reliability(self, component: str) -> Dict[str, Any]:
        # Implement reliability evaluation
        return {
            "uptime": 0,           # Placeholder
            "error_rate": 0,       # Placeholder
            "recovery_time": 0,    # Placeholder
        }

    def _evaluate_functionality(self, component: str) -> Dict[str, Any]:
        # Implement functionality evaluation
        return {
            "feature_coverage": 0,  # Placeholder
            "test_coverage": 0,     # Placeholder
            "bug_density": 0,       # Placeholder
        }

================
File: tools/docs.txt
================
Core Concepts
Tools
Understanding and leveraging tools within the CrewAI framework for agent collaboration and task execution.


Introduction
CrewAI tools empower agents with capabilities ranging from web searching and data analysis to collaboration and delegating tasks among coworkers. This documentation outlines how to create, integrate, and leverage these tools within the CrewAI framework, including a new focus on collaboration tools.


What is a Tool?
A tool in CrewAI is a skill or function that agents can utilize to perform various actions. This includes tools from the CrewAI Toolkit and LangChain Tools, enabling everything from simple searches to complex interactions and effective teamwork among agents.


Key Characteristics of Tools
Utility: Crafted for tasks such as web searching, data analysis, content generation, and agent collaboration.
Integration: Boosts agent capabilities by seamlessly integrating tools into their workflow.
Customizability: Provides the flexibility to develop custom tools or utilize existing ones, catering to the specific needs of agents.
Error Handling: Incorporates robust error handling mechanisms to ensure smooth operation.
Caching Mechanism: Features intelligent caching to optimize performance and reduce redundant operations.

Using CrewAI Tools
To enhance your agents capabilities with crewAI tools, begin by installing our extra tools package:


pip install 'crewai[tools]'
Heres an example demonstrating their use:

Code

import os
from crewai import Agent, Task, Crew
# Importing crewAI tools
from crewai_tools import (
    DirectoryReadTool,
    FileReadTool,
    SerperDevTool,
    WebsiteSearchTool
)

# Set up API keys
os.environ["SERPER_API_KEY"] = "Your Key" # serper.dev API key
os.environ["OPENAI_API_KEY"] = "Your Key"

# Instantiate tools
docs_tool = DirectoryReadTool(directory='./blog-posts')
file_tool = FileReadTool()
search_tool = SerperDevTool()
web_rag_tool = WebsiteSearchTool()

# Create agents
researcher = Agent(
    role='Market Research Analyst',
    goal='Provide up-to-date market analysis of the AI industry',
    backstory='An expert analyst with a keen eye for market trends.',
    tools=[search_tool, web_rag_tool],
    verbose=True
)

writer = Agent(
    role='Content Writer',
    goal='Craft engaging blog posts about the AI industry',
    backstory='A skilled writer with a passion for technology.',
    tools=[docs_tool, file_tool],
    verbose=True
)

# Define tasks
research = Task(
    description='Research the latest trends in the AI industry and provide a summary.',
    expected_output='A summary of the top 3 trending developments in the AI industry with a unique perspective on their significance.',
    agent=researcher
)

write = Task(
    description='Write an engaging blog post about the AI industry, based on the research analysts summary. Draw inspiration from the latest blog posts in the directory.',
    expected_output='A 4-paragraph blog post formatted in markdown with engaging, informative, and accessible content, avoiding complex jargon.',
    agent=writer,
    output_file='blog-posts/new_post.md'  # The final blog post will be saved here
)

# Assemble a crew with planning enabled
crew = Crew(
    agents=[researcher, writer],
    tasks=[research, write],
    verbose=True,
    planning=True,  # Enable planning feature
)

# Execute tasks
crew.kickoff()

Available CrewAI Tools
Error Handling: All tools are built with error handling capabilities, allowing agents to gracefully manage exceptions and continue their tasks.
Caching Mechanism: All tools support caching, enabling agents to efficiently reuse previously obtained results, reducing the load on external resources and speeding up the execution time. You can also define finer control over the caching mechanism using the cache_function attribute on the tool.
Here is a list of the available tools and their descriptions:

Tool	Description
BrowserbaseLoadTool	A tool for interacting with and extracting data from web browsers.
CodeDocsSearchTool	A RAG tool optimized for searching through code documentation and related technical documents.
CodeInterpreterTool	A tool for interpreting python code.
ComposioTool	Enables use of Composio tools.
CSVSearchTool	A RAG tool designed for searching within CSV files, tailored to handle structured data.
DALL-E Tool	A tool for generating images using the DALL-E API.
DirectorySearchTool	A RAG tool for searching within directories, useful for navigating through file systems.
DOCXSearchTool	A RAG tool aimed at searching within DOCX documents, ideal for processing Word files.
DirectoryReadTool	Facilitates reading and processing of directory structures and their contents.
EXASearchTool	A tool designed for performing exhaustive searches across various data sources.
FileReadTool	Enables reading and extracting data from files, supporting various file formats.
FirecrawlSearchTool	A tool to search webpages using Firecrawl and return the results.
FirecrawlCrawlWebsiteTool	A tool for crawling webpages using Firecrawl.
FirecrawlScrapeWebsiteTool	A tool for scraping webpages URL using Firecrawl and returning its contents.
GithubSearchTool	A RAG tool for searching within GitHub repositories, useful for code and documentation search.
SerperDevTool	A specialized tool for development purposes, with specific functionalities under development.
TXTSearchTool	A RAG tool focused on searching within text (.txt) files, suitable for unstructured data.
JSONSearchTool	A RAG tool designed for searching within JSON files, catering to structured data handling.
LlamaIndexTool	Enables the use of LlamaIndex tools.
MDXSearchTool	A RAG tool tailored for searching within Markdown (MDX) files, useful for documentation.
PDFSearchTool	A RAG tool aimed at searching within PDF documents, ideal for processing scanned documents.
PGSearchTool	A RAG tool optimized for searching within PostgreSQL databases, suitable for database queries.
Vision Tool	A tool for generating images using the DALL-E API.
RagTool	A general-purpose RAG tool capable of handling various data sources and types.
ScrapeElementFromWebsiteTool	Enables scraping specific elements from websites, useful for targeted data extraction.
ScrapeWebsiteTool	Facilitates scraping entire websites, ideal for comprehensive data collection.
WebsiteSearchTool	A RAG tool for searching website content, optimized for web data extraction.
XMLSearchTool	A RAG tool designed for searching within XML files, suitable for structured data formats.
YoutubeChannelSearchTool	A RAG tool for searching within YouTube channels, useful for video content analysis.
YoutubeVideoSearchTool	A RAG tool aimed at searching within YouTube videos, ideal for video data extraction.

Creating your own Tools
Developers can craft custom tools tailored for their agents needs or utilize pre-built options.

There are two main ways for one to create a CrewAI tool:


Subclassing BaseTool
Code

from crewai.tools import BaseTool
from pydantic import BaseModel, Field

class MyToolInput(BaseModel):
    """Input schema for MyCustomTool."""
    argument: str = Field(..., description="Description of the argument.")

class MyCustomTool(BaseTool):
    name: str = "Name of my tool"
    description: str = "What this tool does. It's vital for effective utilization."
    args_schema: Type[BaseModel] = MyToolInput

    def _run(self, argument: str) -> str:
        # Your tool's logic here
        return "Tool's result"

Utilizing the tool Decorator
Code

from crewai.tools import tool
@tool("Name of my tool")
def my_tool(question: str) -> str:
    """Clear description for what this tool is useful for, your agent will need this information to use it."""
    # Function logic here
    return "Result from your custom tool"

Structured Tools
The StructuredTool class wraps functions as tools, providing flexibility and validation while reducing boilerplate. It supports custom schemas and dynamic logic for seamless integration of complex functionalities.


Example:
Using StructuredTool.from_function, you can wrap a function that interacts with an external API or system, providing a structured interface. This enables robust validation and consistent execution, making it easier to integrate complex functionalities into your applications as demonstrated in the following example:


from crewai.tools.structured_tool import CrewStructuredTool
from pydantic import BaseModel

# Define the schema for the tool's input using Pydantic
class APICallInput(BaseModel):
    endpoint: str
    parameters: dict

# Wrapper function to execute the API call
def tool_wrapper(*args, **kwargs):
    # Here, you would typically call the API using the parameters
    # For demonstration, we'll return a placeholder string
    return f"Call the API at {kwargs['endpoint']} with parameters {kwargs['parameters']}"

# Create and return the structured tool
def create_structured_tool():
    return CrewStructuredTool.from_function(
        name='Wrapper API',
        description="A tool to wrap API calls with structured input.",
        args_schema=APICallInput,
        func=tool_wrapper,
    )

# Example usage
structured_tool = create_structured_tool()

# Execute the tool with structured input
result = structured_tool._run(**{
    "endpoint": "https://example.com/api",
    "parameters": {"key1": "value1", "key2": "value2"}
})
print(result)  # Output: Call the API at https://example.com/api with parameters {'key1': 'value1', 'key2': 'value2'}

Custom Caching Mechanism
Tools can optionally implement a cache_function to fine-tune caching behavior. This function determines when to cache results based on specific conditions, offering granular control over caching logic.

Code

from crewai.tools import tool

@tool
def multiplication_tool(first_number: int, second_number: int) -> str:
    """Useful for when you need to multiply two numbers together."""
    return first_number * second_number

def cache_func(args, result):
    # In this case, we only cache the result if it's a multiple of 2
    cache = result % 2 == 0
    return cache

multiplication_tool.cache_function = cache_func

writer1 = Agent(
        role="Writer",
        goal="You write lessons of math for kids.",
        backstory="You're an expert in writing and you love to teach kids but you know nothing of math.",
        tools=[multiplication_tool],
        allow_delegation=False,
    )
    #...

Conclusion
Tools are pivotal in extending the capabilities of CrewAI agents, enabling them to undertake a broad spectrum of tasks and collaborate effectively. When building solutions with CrewAI, leverage both custom and existing tools to empower your agents and enhance the AI ecosystem. Consider utilizing error handling, caching mechanisms, and the flexibility of tool arguments to optimize your agents performance and capabilities.

================
File: tools/dynamic_flow_analyzer.py
================
"""Dynamic flow analyzer for generating and optimizing flows."""
from typing import Dict, List, Any, Optional
import json
import requests
from pydantic import BaseModel


class DynamicFlowAnalyzer:
    """Analyzes workspace and generates optimized flows"""
    
    def __init__(self):
        """Initialize the flow analyzer"""
        self.flows = {}
        self.learning_history = []
    
    def analyze_workspace(self, workspace_path: str) -> List[Dict[str, Any]]:
        """Analyze workspace and suggest flows"""
        # TODO: Implement workspace analysis
        return []
    
    def analyze_and_generate_flow(self, requirements: Dict[str, Any], context: Dict[str, Any]) -> str:
        """Generate a flow based on requirements and context"""
        flow_id = f"flow_{len(self.flows)}"
        
        # Create flow with learning-based recommendations
        flow = {
            "id": flow_id,
            "requirements": requirements,
            "context": context,
            "preferred_approach": self._get_preferred_approach(requirements),
            "estimated_duration": self._estimate_duration(requirements),
            "success_criteria": self._extract_success_criteria(requirements)
        }
        
        self.flows[flow_id] = flow
        return flow_id
    
    def get_flow(self, flow_id: str) -> Optional[Dict[str, Any]]:
        """Get a flow by its ID"""
        return self.flows.get(flow_id)
    
    def _get_preferred_approach(self, requirements: Dict[str, Any]) -> str:
        """Get preferred approach based on learning history"""
        # TODO: Implement learning-based approach selection
        return "standard"
    
    def _estimate_duration(self, requirements: Dict[str, Any]) -> str:
        """Estimate duration based on requirements"""
        # TODO: Implement duration estimation
        return "2h"
    
    def _extract_success_criteria(self, requirements: Dict[str, Any]) -> List[str]:
        """Extract success criteria from requirements"""
        return requirements.get("success_factors", [])

================
File: tools/system_tools.py
================
"""System access tools for CrewAI agents."""
from typing import Dict, Any, Optional
from crewai.tools import BaseTool
import logging
from datetime import datetime

class LearningSystemTool(BaseTool):
    """Tool for accessing the learning management system."""
    
    name: str = "learning_system"
    description: str = """Access the learning management system. 
    This tool allows agents to interact with learning resources, track progress, 
    and manage educational content."""
    
    def __init__(self):
        super().__init__()
        self._access_state = {
            "has_access": False,
            "access_level": None,
            "last_verified": None
        }

    def _verify_access(self, agent_role: str) -> bool:
        """Verify if the agent has access to the learning system."""
        # In a real implementation, this would check against actual system permissions
        # For now, we'll simulate basic role-based access
        allowed_roles = ["VP of Engineering", "Learning Manager", "Team Lead"]
        return agent_role in allowed_roles

    async def _run(self, agent_role: str, **kwargs) -> Dict[str, Any]:
        """Required implementation of the abstract _run method."""
        try:
            has_access = self._verify_access(agent_role)
            self._access_state = {
                "has_access": has_access,
                "access_level": "admin" if agent_role == "VP of Engineering" else "user" if has_access else None,
                "last_verified": datetime.now().isoformat()
            }
            return self._access_state
        except Exception as e:
            logging.error(f"Error accessing learning system: {str(e)}")
            return {
                "has_access": False,
                "error": str(e),
                "last_verified": datetime.now().isoformat()
            }

    async def execute(self, agent_role: str, **kwargs) -> Dict[str, Any]:
        """Execute the tool to access the learning system."""
        return await self._run(agent_role, **kwargs)

class ProjectManagementTool(BaseTool):
    """Tool for accessing the project management system."""
    
    name: str = "project_management"
    description: str = """Access the project management system. 
    This tool enables agents to track tasks, manage projects, and coordinate work."""
    
    def __init__(self):
        super().__init__()
        self._access_state = {
            "has_access": False,
            "access_level": None,
            "last_verified": None
        }

    def _verify_access(self, agent_role: str) -> bool:
        """Verify if the agent has access to the project management system."""
        allowed_roles = ["VP of Engineering", "Project Manager", "Team Lead", "Developer"]
        return agent_role in allowed_roles

    async def _run(self, agent_role: str, **kwargs) -> Dict[str, Any]:
        """Required implementation of the abstract _run method."""
        try:
            has_access = self._verify_access(agent_role)
            self._access_state = {
                "has_access": has_access,
                "access_level": "admin" if agent_role == "VP of Engineering" else "user" if has_access else None,
                "last_verified": datetime.now().isoformat()
            }
            return self._access_state
        except Exception as e:
            logging.error(f"Error accessing project management system: {str(e)}")
            return {
                "has_access": False,
                "error": str(e),
                "last_verified": datetime.now().isoformat()
            }

    async def execute(self, agent_role: str, **kwargs) -> Dict[str, Any]:
        """Execute the tool to access the project management system."""
        return await self._run(agent_role, **kwargs)

class CollaborationTool(BaseTool):
    """Tool for accessing collaboration systems."""
    
    name: str = "collaboration_tools"
    description: str = """Access collaboration tools and systems. 
    This tool enables agents to communicate, share resources, and work together effectively."""
    
    def __init__(self):
        super().__init__()
        self._access_state = {
            "has_access": False,
            "access_level": None,
            "last_verified": None
        }

    def _verify_access(self, agent_role: str) -> bool:
        """Verify if the agent has access to collaboration tools."""
        # Most roles should have access to collaboration tools
        return True

    async def _run(self, agent_role: str, **kwargs) -> Dict[str, Any]:
        """Required implementation of the abstract _run method."""
        try:
            has_access = self._verify_access(agent_role)
            self._access_state = {
                "has_access": has_access,
                "access_level": "admin" if agent_role == "VP of Engineering" else "user",
                "last_verified": datetime.now().isoformat()
            }
            return self._access_state
        except Exception as e:
            logging.error(f"Error accessing collaboration tools: {str(e)}")
            return {
                "has_access": False,
                "error": str(e),
                "last_verified": datetime.now().isoformat()
            }

    async def execute(self, agent_role: str, **kwargs) -> Dict[str, Any]:
        """Execute the tool to access collaboration systems."""
        return await self._run(agent_role, **kwargs)

class SystemAccessManager:
    """Manager class for system access tools."""
    
    def __init__(self):
        self.learning_tool = LearningSystemTool()
        self.project_tool = ProjectManagementTool()
        self.collab_tool = CollaborationTool()
        
    def get_tools(self) -> list:
        """Get all system access tools."""
        return [self.learning_tool, self.project_tool, self.collab_tool]
    
    def get_tool(self, tool_name: str) -> Optional[BaseTool]:
        """Get a specific tool by name."""
        tools_map = {
            "learning_system": self.learning_tool,
            "project_management": self.project_tool,
            "collaboration_tools": self.collab_tool
        }
        return tools_map.get(tool_name)

================
File: tools/tool_manager.py
================
from typing import Dict, List, Optional, Any, Type
from .base_tool import BaseTool, ToolMetadata
import inspect
import json
import logging
import os
# Temporarily comment out crewai_tools to avoid OpenAI dependency
# from crewai_tools import (
#     DirectorySearchTool,
#     FileReadTool,
#     # WebsiteSearchTool,  # Will handle in next pass
# )
from pydantic import BaseModel, Field
import asyncio
import threading
import requests

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Get API endpoint from environment
API_ENDPOINT = os.environ.get('AI_API_ENDPOINT', 'https://teqheaidyjmkjwkvkde65rfmo40epndv.lambda-url.eu-west-3.on.aws/')

class ToolExecutionContext(BaseModel):
    """Context for tool execution"""
    agent_id: str
    task_id: Optional[str] = None
    parameters: Dict[str, Any] = Field(default_factory=dict)
    execution_id: str = Field(default_factory=lambda: str(threading.get_ident()))

class DynamicToolManager:
    """Enhanced tool manager with CrewAI integration and dynamic tool creation"""
    
    def __init__(self):
        self._tools: Dict[str, BaseTool] = {}
        self._dynamic_tools: Dict[str, BaseTool] = {}
        self._execution_history: Dict[str, List[Dict[str, Any]]] = {}
        self._lock = threading.Lock()
        self.load_default_tools()

    def load_default_tools(self):
        """Load all default CrewAI tools without OpenAI dependencies"""
        # Temporarily disable all tool loading to avoid OpenAI dependency issues
        logger.info("Tool loading temporarily disabled - using empty tool set")
        self._tools = {}

    def _wrap_crewai_tool(self, name: str, crewai_tool: object) -> List[BaseTool]:
        """Enhanced wrapper for CrewAI tools with better error handling"""
        try:
            tool_methods = inspect.getmembers(crewai_tool, predicate=inspect.ismethod)
            wrapped_tools = []

            for method_name, method in tool_methods:
                if method_name.startswith('_'):
                    continue

                # Create wrapped tool class
                class WrappedTool(BaseTool):
                    def __init__(self, tool_obj, method_name, method):
                        self.tool_obj = tool_obj
                        self.method = method
                        self.method_name = method_name
                        super().__init__()

                    def _get_metadata(self) -> ToolMetadata:
                        doc = self.method.__doc__ or ""
                        return ToolMetadata(
                            name=f"{name}.{self.method_name}",
                            description=doc.strip(),
                            parameters=self._extract_parameters(),
                            return_type="Any",
                            category=name,
                            is_dynamic=False
                        )

                    def _extract_parameters(self) -> Dict:
                        sig = inspect.signature(self.method)
                        return {
                            name: {
                                "type": str(param.annotation),
                                "default": None if param.default is inspect.Parameter.empty else param.default,
                                "description": self._get_param_description(name)
                            }
                            for name, param in sig.parameters.items()
                            if name != 'self'
                        }

                    def _get_param_description(self, param_name: str) -> str:
                        """Extract parameter description from docstring"""
                        doc = self.method.__doc__ or ""
                        param_section = doc.split(":param")
                        for section in param_section[1:]:
                            if section.strip().startswith(f"{param_name}:"):
                                return section.split(":")[1].strip()
                        return f"Parameter {param_name}"

                    async def execute(self, context: ToolExecutionContext, **kwargs):
                        try:
                            # Record execution start
                            self._record_execution_start(context)
                            
                            # Execute the tool
                            result = await self._execute_with_retry(context, **kwargs)
                            
                            # Record successful execution
                            self._record_execution_success(context, result)
                            
                            return result
                        except Exception as e:
                            # Record failed execution
                            self._record_execution_error(context, e)
                            raise

                    async def _execute_with_retry(self, context: ToolExecutionContext, **kwargs):
                        """Execute with retry logic using Lambda endpoint for AI operations"""
                        max_retries = 3
                        retry_delay = 1  # seconds
                        
                        for attempt in range(max_retries):
                            try:
                                # If the tool requires AI capabilities, route through Lambda
                                if getattr(self.method, 'requires_ai', False):
                                    response = requests.post(
                                        API_ENDPOINT,
                                        json={
                                            'type': 'tool_execution',
                                            'tool_name': self.metadata.name,
                                            'parameters': kwargs,
                                            'context': context.dict()
                                        }
                                    )
                                    response.raise_for_status()
                                    return response.json()
                                
                                # Otherwise execute normally
                                if asyncio.iscoroutinefunction(self.method):
                                    return await self.method(**kwargs)
                                return self.method(**kwargs)
                            except Exception as e:
                                if attempt == max_retries - 1:
                                    raise
                                logger.warning(f"Attempt {attempt + 1} failed: {str(e)}")
                                await asyncio.sleep(retry_delay)

                    def _record_execution_start(self, context: ToolExecutionContext):
                        """Record tool execution start"""
                        execution_record = {
                            "execution_id": context.execution_id,
                            "agent_id": context.agent_id,
                            "task_id": context.task_id,
                            "tool_name": self.metadata.name,
                            "parameters": context.parameters,
                            "start_time": asyncio.get_event_loop().time(),
                            "status": "started"
                        }
                        self._add_to_history(execution_record)

                    def _record_execution_success(self, context: ToolExecutionContext, result: Any):
                        """Record successful tool execution"""
                        execution_record = {
                            "execution_id": context.execution_id,
                            "end_time": asyncio.get_event_loop().time(),
                            "status": "completed",
                            "result": str(result)
                        }
                        self._update_history(context.execution_id, execution_record)

                    def _record_execution_error(self, context: ToolExecutionContext, error: Exception):
                        """Record failed tool execution"""
                        execution_record = {
                            "execution_id": context.execution_id,
                            "end_time": asyncio.get_event_loop().time(),
                            "status": "failed",
                            "error": str(error)
                        }
                        self._update_history(context.execution_id, execution_record)

                    def _add_to_history(self, record: Dict[str, Any]):
                        """Add execution record to history"""
                        with self._lock:
                            if self.metadata.name not in self._execution_history:
                                self._execution_history[self.metadata.name] = []
                            self._execution_history[self.metadata.name].append(record)

                    def _update_history(self, execution_id: str, update: Dict[str, Any]):
                        """Update existing execution record"""
                        with self._lock:
                            if self.metadata.name in self._execution_history:
                                for record in self._execution_history[self.metadata.name]:
                                    if record["execution_id"] == execution_id:
                                        record.update(update)
                                        break

                # Create and add wrapped tool
                wrapped_tool = WrappedTool(crewai_tool, method_name, method)
                wrapped_tools.append(wrapped_tool)

            return wrapped_tools
        except Exception as e:
            logger.error(f"Error wrapping CrewAI tool: {str(e)}")
            return []

    async def create_dynamic_tool(self, metadata: ToolMetadata, code: str) -> Optional[BaseTool]:
        """Create a new dynamic tool at runtime with enhanced validation"""
        try:
            # Validate metadata
            if not self._validate_tool_metadata(metadata):
                raise ValueError("Invalid tool metadata")

            # Create tool code with proper structure
            tool_code = self._generate_tool_code(metadata, code)

            # Create namespace and execute code
            namespace = {}
            exec(tool_code, globals(), namespace)

            # Instantiate and validate tool
            tool = namespace["DynamicTool"]()
            if not self._validate_tool(tool):
                raise ValueError("Tool validation failed")

            # Register tool
            with self._lock:
                self._dynamic_tools[metadata.name] = tool
                logger.info(f"Created dynamic tool: {metadata.name}")

            return tool
        except Exception as e:
            logger.error(f"Error creating dynamic tool: {str(e)}")
            return None

    def _validate_tool_metadata(self, metadata: ToolMetadata) -> bool:
        """Validate tool metadata"""
        try:
            # Check required fields
            if not metadata.name or not metadata.description:
                return False

            # Validate parameters
            if metadata.parameters:
                for param, spec in metadata.parameters.items():
                    if not isinstance(spec, dict):
                        return False
                    if "type" not in spec:
                        return False

            return True
        except Exception:
            return False

    def _validate_tool(self, tool: BaseTool) -> bool:
        """Validate tool implementation"""
        try:
            # Check required methods
            if not hasattr(tool, "execute") or not callable(tool.execute):
                return False

            # Validate metadata
            if not tool.metadata or not isinstance(tool.metadata, ToolMetadata):
                return False

            return True
        except Exception:
            return False

    def _generate_tool_code(self, metadata: ToolMetadata, code: str) -> str:
        """Generate tool code with proper structure and error handling"""
        return f"""
class DynamicTool(BaseTool):
    def _get_metadata(self) -> ToolMetadata:
        return ToolMetadata(
            name="{metadata.name}",
            description="{metadata.description}",
            parameters={metadata.parameters},
            return_type="{metadata.return_type}",
            category="{metadata.category}",
            created_by="{metadata.created_by or 'dynamic'}",
            version="{metadata.version}",
            is_dynamic=True
        )

    async def execute(self, context: ToolExecutionContext, **kwargs):
        try:
            # Record execution start
            self._record_execution_start(context)
            
            # Execute the tool
            result = await self._execute_with_retry(context, **kwargs)
            
            # Record successful execution
            self._record_execution_success(context, result)
            
            return result
        except Exception as e:
            # Record failed execution
            self._record_execution_error(context, e)
            raise

    async def _execute_with_retry(self, context: ToolExecutionContext, **kwargs):
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
{code}
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                await asyncio.sleep(retry_delay)
        """

    def register_tool(self, tool: BaseTool):
        """Register a new tool with validation"""
        try:
            if not self._validate_tool(tool):
                raise ValueError("Invalid tool")

            with self._lock:
                if tool.metadata.is_dynamic:
                    self._dynamic_tools[tool.metadata.name] = tool
                else:
                    self._tools[tool.metadata.name] = tool
                logger.info(f"Registered tool: {tool.metadata.name}")
        except Exception as e:
            logger.error(f"Error registering tool: {str(e)}")

    def unregister_tool(self, tool_name: str):
        """Unregister a tool"""
        try:
            with self._lock:
                if tool_name in self._dynamic_tools:
                    del self._dynamic_tools[tool_name]
                    logger.info(f"Unregistered dynamic tool: {tool_name}")
                elif tool_name in self._tools:
                    del self._tools[tool_name]
                    logger.info(f"Unregistered tool: {tool_name}")
        except Exception as e:
            logger.error(f"Error unregistering tool: {str(e)}")

    def get_tool(self, tool_name: str) -> Optional[BaseTool]:
        """Get a tool by name"""
        return self._tools.get(tool_name) or self._dynamic_tools.get(tool_name)

    def list_tools(self, include_dynamic: bool = True) -> List[Dict[str, Any]]:
        """List all available tools"""
        try:
            tools_list = [tool.to_dict() for tool in self._tools.values()]
            if include_dynamic:
                tools_list.extend([tool.to_dict() for tool in self._dynamic_tools.values()])
            return tools_list
        except Exception as e:
            logger.error(f"Error listing tools: {str(e)}")
            return []

    def get_tools_description(self) -> str:
        """Get a formatted description of all tools for LLM consumption"""
        try:
            descriptions = []
            for tool in self.list_tools():
                desc = f"Tool: {tool['name']}\n"
                desc += f"Description: {tool['description']}\n"
                desc += f"Parameters: {json.dumps(tool['parameters'], indent=2)}\n"
                desc += f"Returns: {tool['return_type']}\n"
                desc += f"Category: {tool['category']}\n"
                desc += "-" * 50
                descriptions.append(desc)
            return "\n".join(descriptions)
        except Exception as e:
            logger.error(f"Error getting tools description: {str(e)}")
            return "Error retrieving tools description"

    def get_execution_history(self, tool_name: Optional[str] = None) -> List[Dict[str, Any]]:
        """Get execution history for a tool or all tools"""
        try:
            with self._lock:
                if tool_name:
                    return self._execution_history.get(tool_name, [])
                return [
                    record
                    for records in self._execution_history.values()
                    for record in records
                ]
        except Exception as e:
            logger.error(f"Error getting execution history: {str(e)}")
            return []

    def clear_execution_history(self, tool_name: Optional[str] = None):
        """Clear execution history for a tool or all tools"""
        try:
            with self._lock:
                if tool_name:
                    self._execution_history.pop(tool_name, None)
                else:
                    self._execution_history.clear()
        except Exception as e:
            logger.error(f"Error clearing execution history: {str(e)}")

================
File: ui/__init__.py
================
"""
Tribe UI module - Contains user interface related functionality
"""

================
File: __init__.py
================
"""
Tribe extension for VS Code
"""

__version__ = "0.1.0"

from .crew import Tribe

================
File: .gitignore
================
.env
__pycache__/
.DS_Store

================
File: crew.py
================
"""
Tribe - Main crew implementation for the Tribe framework
"""

from typing import Dict, Any, Optional
from crewai import Crew, Agent, Task, Process
from .core.dynamic import DynamicCrew, DynamicAgent, ProjectManager
from .core.crew_collab import CollaborationMode
import os
import asyncio
import logging
import uuid

class Tribe:
    """
    Main class for managing AI agent crews in the Tribe framework.
    Provides high-level interface for creating and managing agent crews.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize a new Tribe instance.
        
        Args:
            config (Optional[Dict[str, Any]]): Configuration dictionary for the tribe
        """
        self.config = config or {}
        
        # Get API endpoint from environment or config
        self.api_endpoint = (
            os.environ.get('AI_API_ENDPOINT') or 
            config.get('api_endpoint') or
            "https://teqheaidyjmkjwkvkde65rfmo40epndv.lambda-url.eu-west-3.on.aws/"
        )
        logging.info(f"Using API endpoint from environment: {self.api_endpoint}")
        
        # Update config with API endpoint
        self.config['api_endpoint'] = self.api_endpoint
        
        # Initialize event loop
        try:
            self._init_loop = asyncio.get_event_loop()
        except RuntimeError:
            self._init_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self._init_loop)
        
        self._initialized = False
        self._crew = None
        self._project_manager = None
        
    async def initialize(self):
        """Async initialization of Tribe"""
        if self._initialized:
            return
            
        try:
            logging.info("Creating Tribe instance with config...")
            
            # Create VP of Engineering with API endpoint
            vp_agent = await DynamicAgent.create_vp_engineering(project_description="Tribe System")
            
            # Initialize project manager for enhanced task execution
            self._project_manager = ProjectManager()
            
            # Initialize DynamicCrew with minimal config
            crew_config = {
                'api_endpoint': self.api_endpoint,
                'debug': self.config.get('debug', False),
                'project_manager': self._project_manager,
                'team_creation': {
                    'max_retries': 3,
                    'retry_delay': 1.0,
                    'validation_timeout': 30.0,
                    'min_agents_required': 1,
                    'max_team_size': 10
                }
            }
            
            self._crew = DynamicCrew(config=crew_config)
            self._crew._agent_pool = []  # Initialize agent pool
            
            # Add VP of Engineering to crew
            self._crew.add_agent(vp_agent)
            
            # Create setup task
            setup_task = Task(
                description="Initialize Tribe system and verify configuration",
                expected_output="Initialized system state",
                agent=self._crew.get_active_agents()[0]  # Use VP of Engineering
            )
            
            # Initialize crew and execute setup task
            team_id = str(uuid.uuid4())
            self._initialized = True
            
            return {
                "team": {
                    "id": team_id,
                    "description": "Tribe System",
                    "agents": [
                        {
                            "id": str(uuid.uuid4()),
                            "role": agent.role,
                            "status": "active"
                        }
                        for agent in self._crew.get_active_agents()
                    ]
                }
            }
            
        except Exception as e:
            logging.error(f"Error initializing Tribe: {str(e)}")
            raise
    
    @classmethod
    async def create(cls, config: Optional[Dict[str, Any]] = None) -> 'Tribe':
        """Factory method to create and initialize a Tribe instance"""
        instance = cls(config)
        await instance.initialize()
        return instance
    
    @property
    def crew(self) -> DynamicCrew:
        """Get the DynamicCrew instance."""
        if not self._initialized:
            raise RuntimeError("Tribe instance not initialized. Call initialize() first.")
        return self._crew
        
    def __del__(self):
        """Cleanup when Tribe instance is destroyed"""
        if hasattr(self, '_init_loop') and self._init_loop:
            try:
                self._init_loop.close()
            except Exception:
                pass

================
File: extension.py
================
"""VS Code extension entry point for Tribe."""
import os
import json
import asyncio
import requests
import uuid
import logging
import difflib
from pygls.server import LanguageServer
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime

class TribeLanguageServer(LanguageServer):
    def __init__(self):
        super().__init__()
        self.workspace_path = None
        self.ai_api_endpoint = os.getenv("AI_API_ENDPOINT", "https://teqheaidyjmkjwkvkde65rfmo40epndv.lambda-url.eu-west-3.on.aws/")
        if not self.ai_api_endpoint:
            self.ai_api_endpoint = "https://teqheaidyjmkjwkvkde65rfmo40epndv.lambda-url.eu-west-3.on.aws/"

tribe_server = TribeLanguageServer()

@tribe_server.command('tribe.initialize')
def initialize_tribe(ls: TribeLanguageServer, workspace_path: str) -> None:
    """Initialize Tribe with the workspace path."""
    ls.workspace_path = workspace_path

@tribe_server.command('tribe.createTeam')
async def create_team(ls: TribeLanguageServer, payload: dict) -> dict:
    """Create a new team using the DynamicAgent through Tribe."""
    try:
        from tribe import Tribe
        
        # Initialize Tribe with minimal configuration
        tribe = await Tribe.create(config={
            'api_endpoint': ls.ai_api_endpoint
        })
        
        # Get the team structure from the initialized crew
        result = {
            "team": {
                "id": str(uuid.uuid4()),
                "description": "Initial Tribe Team",
                "agents": [
                    {
                        "id": str(uuid.uuid4()),
                        "role": agent.role,
                        "status": "active"
                    }
                    for agent in tribe.crew.get_active_agents()
                ]
            }
        }
        
        return result
        
    except Exception as e:
        logging.error(f"Error creating team: {str(e)}")
        return {"error": f"Error creating team: {str(e)}"}

@tribe_server.command('tribe.initializeProject')
def initialize_project(ls: TribeLanguageServer, payload: dict) -> dict:
    """Initialize a project with the created team data."""
    try:
        response = requests.post(
            ls.ai_api_endpoint,
            json={
                "messages": [
                    {
                        "role": "system",
                        "content": "You are the VP of Engineering, responsible for analyzing project requirements and creating the optimal team of AI agents, describing the initial set of tools, assigning tasks to the team, and creating initial workflow."
                    },
                    {
                        "role": "user",
                        "content": payload.get("description", "")
                    }
                ]
            }
        )
        data = response.json()
        return {
            "id": str(int(asyncio.get_event_loop().time() * 1000)),
            "vision": payload.get("description", ""),
            "response": data.get("choices", [{}])[0].get("message", {}).get("content", "")
        }
    except Exception as e:
        return {"error": str(e)}

@tribe_server.command('tribe.createAgent')
def create_agent(ls: TribeLanguageServer, spec: dict) -> dict:
    """Create a new agent."""
    try:
        response = requests.post(
            f"{ls.ai_api_endpoint}/agent/create",
            json={
                "name": spec.get("name"),
                "role": spec.get("role"),
                "backstory": spec.get("backstory")
            }
        )
        data = response.json()
        return data.get("agent") or {
            "id": "",
            "name": spec.get("name"),
            "role": spec.get("role"),
            "status": "error",
            "backstory": spec.get("backstory")
        }
    except Exception as e:
        return {"error": str(e)}

@tribe_server.command('tribe.getAgents')
def get_agents(ls: TribeLanguageServer) -> list:
    """Get all agents for current project."""
    try:
        response = requests.get(f"{ls.ai_api_endpoint}/agents")
        data = response.json()
        return data.get("agents", [])
    except Exception as e:
        return []

@tribe_server.command('tribe.sendAgentMessage')
async def send_agent_message(ls: TribeLanguageServer, payload: dict) -> dict:
    """Send a message to a specific agent."""
    try:
        logging.info(f"Sending message to agent. Payload: {json.dumps(payload)}")
        
        # Determine if this is a self-referential query
        is_self_referential = any(keyword in payload.get("message", "").lower() 
                                for keyword in ["your role", "your capabilities", "what can you do", "who are you",
                                              "access", "system", "learning", "project management"])
        
        # Get agent's role context
        agent_id = payload.get("agentId")
        agent = ls.get_agent(agent_id)
        role_context = agent.get_role_context() if agent else {}
        
        # If this is a system access query, verify current access status using tools
        if is_self_referential and agent:
            # Get access status using the agent's tools
            access_status = {}
            for system in ["learning_system", "project_management", "collaboration_tools"]:
                access_status[system] = await agent.verify_system_access(system)
            role_context["system_access_status"] = access_status
        
        # Format the request with the correct Lambda message structure
        request_payload = {
            "messages": [
                {
                    "role": "system",
                    "content": f"""You are an AI agent with the role of {payload.get('agentId')}. 
                    Your responses should be based on your actual system access and capabilities.
                    You have access to the following tools:
                    {', '.join(tool.name for tool in agent.tools) if agent else 'No tools available'}
                    
                    When discussing system access, only claim access to systems that are verified through your tools."""
                },
                {
                    "role": "user",
                    "content": payload.get("message")
                }
            ],
            "roleContext": role_context,
            "isSelfReferential": is_self_referential
        }
        
        # If self-referential, use specialized handling
        if is_self_referential and agent:
            response_content = await agent.handle_self_referential_query(payload.get("message"))
        else:
            response = requests.post(
                ls.ai_api_endpoint,
                json=request_payload
            )
            if not response.ok:
                logging.error(f"API request failed: {response.status_code}")
                return {
                    "type": "ERROR",
                    "payload": f"API request failed: {response.status_code}"
                }
            response_content = response.text.strip('"')
            
        message_response = {
            "type": "MESSAGE_RESPONSE",
            "payload": {
                "id": str(uuid.uuid4()),
                "sender": payload.get("agentId", "Unknown Agent"),
                "content": response_content,
                "timestamp": datetime.now().isoformat(),
                "type": "agent",
                "targetAgent": payload.get("agentId"),
                "isVPResponse": payload.get("isVPMessage", False),
                "isManagerResponse": payload.get("isTeamMessage", False),
                "isSelfReferential": is_self_referential,
                "systemAccessVerified": True if agent and is_self_referential else False,
                "availableTools": [tool.name for tool in agent.tools] if agent else []
            }
        }
        
        logging.info(f"Sending message response: {json.dumps(message_response)}")
        return message_response
        
    except requests.RequestException as e:
        error_msg = f"Network error while sending message: {str(e)}"
        logging.error(error_msg)
        return {"type": "ERROR", "payload": error_msg}
    except Exception as e:
        error_msg = f"Unexpected error while sending message: {str(e)}"
        logging.error(error_msg)
        return {"type": "ERROR", "payload": error_msg}

@tribe_server.command('tribe.sendCrewMessage')
def send_crew_message(ls: TribeLanguageServer, payload: dict) -> dict:
    """Send a message to the entire team."""
    try:
        response = requests.post(
            f"{ls.ai_api_endpoint}/team/message",
            json={"message": payload.get("message")}
        )
        data = response.json()
        if "error" in data:
            return {"error": data["error"]}
        
        # Process responses from all agents
        messages = []
        for agent_response in data.get("responses", []):
            messages.append({
                "id": str(uuid.uuid4()),
                "sender": agent_response.get("agentId", "Unknown Agent"),
                "content": agent_response.get("response", "No response"),
                "timestamp": agent_response.get("timestamp", datetime.now().isoformat()),
                "type": "agent"
            })
        
        return {
            "type": "MESSAGE_RESPONSE",
            "payload": messages
        }
    except Exception as e:
        return {"error": str(e)}

@tribe_server.command('tribe.analyzeRequirements')
def analyze_requirements(ls: TribeLanguageServer, payload: dict) -> str:
    """Analyze requirements and create/update agents."""
    try:
        response = requests.post(
            f"{ls.ai_api_endpoint}/requirements/analyze",
            json={
                "requirements": payload.get("requirements")
            }
        )
        data = response.json()
        return data.get("analysis") or f"Analysis failed for requirements:\n{payload.get('requirements')}\n\nPlease try again with more detailed requirements."
    except Exception as e:
        return str(e)

@tribe_server.command('tribe.createTask')
def create_task(ls: TribeLanguageServer, payload: dict) -> dict:
    """Create a new task."""
    try:
        response = requests.post(
            f"{ls.ai_api_endpoint}/task/create",
            json={
                "type": "create_task",
                "description": payload.get("description"),
                "assigned_to": payload.get("assignedTo"),
                "name": payload.get("name")
            }
        )
        data = response.json()
        return data.get("task") or {
            "id": "",
            "name": payload.get("name"),
            "status": "error",
            "assignedTo": payload.get("assignedTo"),
            "description": payload.get("description")
        }
    except Exception as e:
        return {"error": str(e)}

@tribe_server.command('tribe.recordFeedback')
def record_feedback(ls: TribeLanguageServer, payload: dict) -> dict:
    """Record agent feedback."""
    try:
        response = requests.post(
            f"{ls.ai_api_endpoint}/feedback/record",
            json={
                "type": "record_feedback",
                "agent_id": payload.get("agentId"),
                "action_type": payload.get("actionType"),
                "feedback": payload.get("feedback"),
                "context": payload.get("context"),
                "accepted": payload.get("accepted")
            }
        )
        data = response.json()
        return {"result": data} or {
            "status": "error",
            "message": "Failed to record feedback"
        }
    except Exception as e:
        return {"error": str(e)}

@tribe_server.command('tribe.setAutonomyLevel')
def set_autonomy_level(ls: TribeLanguageServer, payload: dict) -> dict:
    """Set agent autonomy level."""
    try:
        response = requests.post(
            f"{ls.ai_api_endpoint}/autonomy/set",
            json={
                "type": "set_autonomy",
                "agent_id": payload.get("agentId"),
                "task_type": payload.get("taskType"),
                "autonomy_level": payload.get("autonomyLevel"),
                "supervision_requirements": payload.get("supervisionRequirements")
            }
        )
        data = response.json()
        return {"result": data} or {
            "status": "error",
            "message": "Failed to set autonomy level",
            "agent_id": payload.get("agentId"),
            "autonomy_level": payload.get("autonomyLevel")
        }
    except Exception as e:
        return {"error": str(e)}

@tribe_server.command('tribe.analyzeFlows')
def analyze_flows(ls: TribeLanguageServer) -> dict:
    """Analyze workflow flows."""
    try:
        response = requests.get(f"{ls.ai_api_endpoint}/flows/analyze")
        data = response.json()
        return {"suggestions": data.get("suggestions", [])} or {
            "suggestions": [],
            "status": "error",
            "message": "Failed to analyze flows"
        }
    except Exception as e:
        return {"error": str(e)}

@tribe_server.command('tribe.generateFlow')
def generate_flow(ls: TribeLanguageServer, payload: dict) -> dict:
    """Generate new workflow flow."""
    try:
        response = requests.post(
            f"{ls.ai_api_endpoint}/flow/generate",
            json={
                "requirements": payload.get("requirements"),
                "context": payload.get("context")
            }
        )
        data = response.json()
        flow_id = data.get("flow_id")
        steps = data.get("steps", [])
        return {
            "flow_id": flow_id,
            "steps": steps,
            "requirements": payload.get("requirements"),
            "context": payload.get("context")
        } or {
            "flow_id": "",
            "steps": [],
            "requirements": payload.get("requirements"),
            "context": payload.get("context"),
            "status": "error",
            "message": "Failed to generate flow"
        }
    except Exception as e:
        return {"error": str(e)}

@tribe_server.command('tribe.executeFlow')
def execute_flow(ls: TribeLanguageServer, payload: dict) -> dict:
    """Execute workflow flow."""
    try:
        response = requests.post(
            f"{ls.ai_api_endpoint}/flow/execute",
            json={
                "flow_id": payload.get("flowId"),
                "initial_state": payload.get("initialState")
            }
        )
        data = response.json()
        return {
            "result": data.get("result"),
            "state": data.get("state", {}),
            "visualizations": data.get("visualizations", []),
            "proposed_changes": {
                "files_to_modify": data.get("files_to_modify", []),
                "files_to_create": data.get("files_to_create", []),
                "files_to_delete": data.get("files_to_delete", [])
            }
        } or {
            "result": None,
            "state": payload.get("initialState"),
            "visualizations": [],
            "proposed_changes": {
                "files_to_modify": [],
                "files_to_create": [],
                "files_to_delete": []
            },
            "status": "error",
            "message": "Failed to execute flow"
        }
    except Exception as e:
        return {"error": str(e)}

@tribe_server.command('tribe.createWorkflow')
def create_workflow(ls: TribeLanguageServer, payload: dict) -> dict:
    """Create new workflow."""
    try:
        response = requests.post(
            f"{ls.ai_api_endpoint}/workflow/create",
            json={
                "name": payload.get("name"),
                "description": payload.get("description"),
                "steps": payload.get("steps"),
                "checkpoints": payload.get("checkpoints"),
                "required_approvals": payload.get("requiredApprovals")
            }
        )
        data = response.json()
        return {"workflow": data} or {
            "id": "",
            "name": payload.get("name"),
            "description": payload.get("description"),
            "steps": payload.get("steps"),
            "checkpoints": payload.get("checkpoints"),
            "required_approvals": payload.get("requiredApprovals"),
            "status": "error",
            "message": "Failed to create workflow"
        }
    except Exception as e:
        return {"error": str(e)}

@tribe_server.command('tribe.generateCode')
def generate_code(ls: TribeLanguageServer, payload: dict) -> str:
    """Generate code based on requirements."""
    try:
        response = requests.post(
            f"{ls.ai_api_endpoint}/code/generate",
            json={
                "task_type": "code_generation",
                "requirements": payload.get("requirements"),
                "language": payload.get("language"),
                "framework": payload.get("framework", ""),
                "output_file": payload.get("outputFile")
            }
        )
        data = response.json()
        if data.get("result") and data["result"].get("code"):
            return data["result"]["code"]
        return f"// Failed to generate code for requirements:\n// {payload.get('requirements')}\n// Language: {payload.get('language')}\n// Framework: {payload.get('framework', 'none')}"
    except Exception as e:
        return str(e)

# Enhanced diff and code management commands

@tribe_server.command('tribe.calculateDetailedDiff')
def calculate_detailed_diff(ls: TribeLanguageServer, payload: dict) -> dict:
    """Calculate a detailed diff between two content versions using Myers diff algorithm."""
    try:
        old_content = payload.get("oldContent", "")
        new_content = payload.get("newContent", "")
        file_path = payload.get("filePath", "")
        
        # Split content into lines
        old_lines = old_content.splitlines()
        new_lines = new_content.splitlines()
        
        # Use Python's difflib for diff calculation (similar to Myers algorithm)
        differ = difflib.SequenceMatcher(None, old_lines, new_lines)
        
        # Generate hunks from the diff
        hunks = []
        for tag, i1, i2, j1, j2 in differ.get_opcodes():
            if tag != 'equal':
                hunk = {
                    "startLine": i1 + 1,  # Convert to 1-indexed
                    "endLine": i2,
                    "content": "\n".join(new_lines[j1:j2]),
                    "originalContent": "\n".join(old_lines[i1:i2]),
                    "semanticGroup": "default"  # Default group, would be enhanced in a real implementation
                }
                hunks.append(hunk)
        
        # Create file change object
        file_change = {
            "path": file_path,
            "content": new_content,
            "originalContent": old_content,
            "timestamp": datetime.now().isoformat(),
            "hunks": hunks
        }
        
        return {
            "fileChange": file_change,
            "success": True
        }
    except Exception as e:
        logging.error(f"Error calculating detailed diff: {str(e)}")
        return {
            "error": str(e),
            "success": False
        }

@tribe_server.command('tribe.detectConflicts')
def detect_conflicts(ls: TribeLanguageServer, payload: dict) -> dict:
    """Detect conflicts in changes from multiple agents."""
    try:
        changes = payload.get("changes", [])
        conflicts = []
        
        # Group changes by file path
        file_changes = {}
        
        # Collect all file modifications
        for change in changes:
            agent_id = change.get("agentId", "unknown")
            agent_name = change.get("agentName", "Unknown Agent")
            
            for file in change.get("files", {}).get("modify", []):
                path = file.get("path", "")
                if not path:
                    continue
                    
                if path not in file_changes:
                    file_changes[path] = []
                
                file_changes[path].append({
                    "agentId": agent_id,
                    "agentName": agent_name,
                    "content": file.get("content", ""),
                    "originalContent": file.get("originalContent", "")
                })
        
        # Detect conflicts in file modifications
        for file_path, changes in file_changes.items():
            if len(changes) > 1:
                # Check if the changes are identical
                first_content = changes[0].get("content", "")
                has_conflict = any(change.get("content", "") != first_content for change in changes)
                
                if has_conflict:
                    # Create a conflict
                    conflict_id = f"conflict-{int(datetime.now().timestamp())}-{uuid.uuid4().hex[:8]}"
                    conflicting_changes = {}
                    
                    for change in changes:
                        agent_id = change.get("agentId", "unknown")
                        if agent_id not in conflicting_changes:
                            conflicting_changes[agent_id] = []
                        
                        conflicting_changes[agent_id].append({
                            "path": file_path,
                            "content": change.get("content", ""),
                            "originalContent": change.get("originalContent", ""),
                            "timestamp": datetime.now().isoformat()
                        })
                    
                    conflict = {
                        "id": conflict_id,
                        "type": "merge",
                        "description": f"Multiple agents modified {file_path}",
                        "status": "pending",
                        "files": [file_path],
                        "conflictingChanges": conflicting_changes
                    }
                    
                    conflicts.append(conflict)
                    
                    # In a real implementation, we would save the conflict to storage here
        
        return {
            "conflicts": conflicts,
            "success": True
        }
    except Exception as e:
        logging.error(f"Error detecting conflicts: {str(e)}")
        return {
            "error": str(e),
            "success": False
        }

@tribe_server.command('tribe.resolveConflict')
def resolve_conflict(ls: TribeLanguageServer, payload: dict) -> dict:
    """Resolve a conflict either automatically or manually."""
    try:
        conflict_id = payload.get("conflictId", "")
        resolution = payload.get("resolution", "auto")
        manual_resolution = payload.get("manualResolution", [])
        
        if not conflict_id:
            return {
                "error": "Missing conflict ID",
                "success": False
            }
        
        # In a real implementation, we would retrieve the conflict from storage
        # For this example, we'll simulate the resolution
        
        if resolution == "auto":
            # Simulate automatic conflict resolution
            # In a real implementation, we would use a more sophisticated algorithm
            resolved_changes = []
            
            # For demonstration purposes, we'll just return a success response
            return {
                "resolvedChanges": resolved_changes,
                "success": True
            }
        elif resolution == "manual" and manual_resolution:
            # Apply the manual resolution
            resolved_changes = []
            
            for file in manual_resolution:
                resolved_changes.append({
                    "path": file.get("path", ""),
                    "content": file.get("content", ""),
                    "timestamp": datetime.now().isoformat()
                })
            
            return {
                "resolvedChanges": resolved_changes,
                "success": True
            }
        else:
            return {
                "error": "Invalid resolution strategy or missing manual resolution",
                "success": False
            }
    except Exception as e:
        logging.error(f"Error resolving conflict: {str(e)}")
        return {
            "error": str(e),
            "success": False
        }

@tribe_server.command('tribe.createCheckpoint')
def create_checkpoint(ls: TribeLanguageServer, payload: dict) -> dict:
    """Create a checkpoint of the current workspace state."""
    try:
        description = payload.get("description", "Checkpoint")
        change_groups = payload.get("changeGroups", [])
        
        # In a real implementation, we would create a snapshot of the workspace
        # For this example, we'll simulate the checkpoint creation
        
        checkpoint_id = f"checkpoint-{int(datetime.now().timestamp())}"
        checkpoint = {
            "id": checkpoint_id,
            "timestamp": datetime.now().isoformat(),
            "description": description,
            "changes": {
                "modified": 0,
                "created": 0,
                "deleted": 0
            },
            "snapshotPath": f"/tmp/{checkpoint_id}.json",
            "changeGroups": change_groups
        }
        
        return {
            "checkpoint": checkpoint,
            "success": True
        }
    except Exception as e:
        logging.error(f"Error creating checkpoint: {str(e)}")
        return {
            "error": str(e),
            "success": False
        }

@tribe_server.command('tribe.createSubCheckpoint')
def create_sub_checkpoint(ls: TribeLanguageServer, payload: dict) -> dict:
    """Create a sub-checkpoint within a parent checkpoint."""
    try:
        parent_checkpoint_id = payload.get("parentCheckpointId", "")
        description = payload.get("description", "Sub-checkpoint")
        changes = payload.get("changes", {})
        
        if not parent_checkpoint_id:
            return {
                "error": "Missing parent checkpoint ID",
                "success": False
            }
        
        # In a real implementation, we would verify the parent checkpoint exists
        # and create the sub-checkpoint
        
        sub_checkpoint = {
            "id": f"sub-{int(datetime.now().timestamp())}",
            "timestamp": datetime.now().isoformat(),
            "description": description,
            "parentCheckpointId": parent_checkpoint_id,
            "changes": changes
        }
        
        return {
            "subCheckpoint": sub_checkpoint,
            "success": True
        }
    except Exception as e:
        logging.error(f"Error creating sub-checkpoint: {str(e)}")
        return {
            "error": str(e),
            "success": False
        }

@tribe_server.command('tribe.revertToSubCheckpoint')
def revert_to_sub_checkpoint(ls: TribeLanguageServer, payload: dict) -> dict:
    """Revert to a specific sub-checkpoint."""
    try:
        parent_checkpoint_id = payload.get("parentCheckpointId", "")
        sub_checkpoint_id = payload.get("subCheckpointId", "")
        
        if not parent_checkpoint_id or not sub_checkpoint_id:
            return {
                "error": "Missing parent checkpoint ID or sub-checkpoint ID",
                "success": False
            }
        
        # In a real implementation, we would retrieve the sub-checkpoint
        # and apply the changes to revert to that state
        
        return {
            "success": True
        }
    except Exception as e:
        logging.error(f"Error reverting to sub-checkpoint: {str(e)}")
        return {
            "error": str(e),
            "success": False
        }

@tribe_server.command('tribe.getChangeHistory')
def get_change_history(ls: TribeLanguageServer) -> dict:
    """Get the change history."""
    try:
        # In a real implementation, we would retrieve the change history from storage
        # For this example, we'll return an empty history
        
        return {
            "history": [],
            "success": True
        }
    except Exception as e:
        logging.error(f"Error getting change history: {str(e)}")
        return {
            "error": str(e),
            "success": False
        }

@tribe_server.command('tribe.groupChangesByFeature')
def group_changes_by_feature(ls: TribeLanguageServer, payload: dict) -> dict:
    """Group file changes by semantic features."""
    try:
        file_changes = payload.get("fileChanges", [])
        
        # Group by file extension as a simple heuristic
        grouped_changes = {}
        
        for change in file_changes:
            path = change.get("path", "")
            if not path:
                continue
                
            # Extract file extension
            _, ext = os.path.splitext(path)
            ext = ext[1:] if ext else "other"  # Remove the dot
            
            if ext not in grouped_changes:
                grouped_changes[ext] = []
                
            grouped_changes[ext].append(change)
        
        # If we have hunks with semantic groups, use those for more detailed grouping
        semantic_groups = {}
        
        for change in file_changes:
            hunks = change.get("hunks", [])
            for hunk in hunks:
                semantic_group = hunk.get("semanticGroup")
                if semantic_group:
                    if semantic_group not in semantic_groups:
                        semantic_groups[semantic_group] = []
                    
                    # Check if we already have this file change in this group
                    path = change.get("path", "")
                    existing_change = next((c for c in semantic_groups[semantic_group] if c.get("path") == path), None)
                    
                    if existing_change:
                        # Add the hunk to the existing change
                        if "hunks" not in existing_change:
                            existing_change["hunks"] = []
                        existing_change["hunks"].append(hunk)
                    else:
                        # Create a new file change with just this hunk
                        semantic_groups[semantic_group].append({
                            "path": path,
                            "content": change.get("content", ""),
                            "originalContent": change.get("originalContent", ""),
                            "hunks": [hunk]
                        })
        
        # Merge the extension-based groups and semantic groups
        result = {**grouped_changes, **semantic_groups}
        
        # If we have no groups, add an "Uncategorized" group with all changes
        if not result:
            result["Uncategorized"] = file_changes
        
        return {
            "groupedChanges": result,
            "success": True
        }
    except Exception as e:
        logging.error(f"Error grouping changes by feature: {str(e)}")
        return {
            "error": str(e),
            "success": False
        }

================
File: lambda.ts
================
/* eslint-disable header/header */
import { GetSecretValueCommand, SecretsManagerClient } from '@aws-sdk/client-secrets-manager';

const secretsClient = new SecretsManagerClient({
	region: process.env.AWS_REGION || 'eu-west-3',
});

async function getOpenRouterKey() {
	const command = new GetSecretValueCommand({
		SecretId: 'OpenRouter-MightDev-API-Key',
	});

	const response = await secretsClient.send(command);
	if ('SecretString' in response) {
		const secret = JSON.parse(response.SecretString);
		return secret.open_router_api_key;
	}
	throw new Error('OpenRouter API key not found');
}

export const handler = async (event) => {
	console.log('Received event:', JSON.stringify(event));
	try {
		// Parse the body from the event
		const body = typeof event.body === 'string' ? JSON.parse(event.body) : event.body;

		// Extract messages from either the direct format or nested format for backward compatibility
		const messages = body?.messages || body?.body?.messages;

		if (!messages || !Array.isArray(messages)) {
			throw new Error('Invalid request format: messages array is required');
		}

		// Extract role context and self-reference flags
		const roleContext = body?.roleContext || {};
		const isSelfReferential = body?.isSelfReferential || false;

		// Add role context to system message if present
		const systemMessage = messages.find((m) => m.role === 'system');
		if (systemMessage && roleContext) {
			// Include system access information in the context
			const systemAccessInfo = roleContext.system_access || {
				learning_system: { has_access: false, access_level: null, last_verified: null },
				project_management: { has_access: false, access_level: null, last_verified: null },
				collaboration_tools: { has_access: false, access_level: null, last_verified: null },
			};

			const contextualizedContent = `${systemMessage.content}\n\nCurrent System Access Status:
- Learning System: ${JSON.stringify(systemAccessInfo.learning_system)}
- Project Management: ${JSON.stringify(systemAccessInfo.project_management)}
- Collaboration Tools: ${JSON.stringify(systemAccessInfo.collaboration_tools)}

Role Context: ${JSON.stringify(roleContext)}`;

			systemMessage.content = contextualizedContent;

			if (isSelfReferential) {
				systemMessage.content += '\nThis is a self-referential question about your role and capabilities.';
			}
		}

		// Get API key from Secrets Manager
		const apiKey = await getOpenRouterKey();

		// Call OpenRouter API
		const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
			method: 'POST',
			headers: {
				Authorization: `Bearer ${apiKey}`,
				'Content-Type': 'application/json',
				'HTTP-Referer': process.env.REFERER_URL || 'http://localhost:3000',
			},
			body: JSON.stringify({
				model: 'openrouter/auto',
				route: 'fallback',
				messages: messages,
				temperature: 0.7,
				max_tokens: 2500,
			}),
		});

		if (!response.ok) {
			const errorText = await response.text();
			console.error('OpenRouter API error:', response.status, errorText);
			throw new Error(`OpenRouter API error: ${response.status}`);
		}

		const data = await response.json();
		console.log('OpenRouter API response:', JSON.stringify(data));

		if (!data.choices || !data.choices[0] || !data.choices[0].message) {
			throw new Error('Invalid response format from OpenRouter API');
		}

		// Return the response in our expected format
		return {
			statusCode: 200,
			headers: {
				'Content-Type': 'application/json',
				'Access-Control-Allow-Origin': process.env.ALLOWED_ORIGIN || '*',
				'Cache-Control': 'no-cache',
			},
			body: JSON.stringify(data.choices[0].message.content),
		};
	} catch (error) {
		console.error('Error:', error);

		return {
			statusCode: error.status || 500,
			headers: {
				'Content-Type': 'application/json',
				'Access-Control-Allow-Origin': process.env.ALLOWED_ORIGIN || '*',
				'Cache-Control': 'no-cache',
			},
			body: JSON.stringify({
				error: error.message,
				details: process.env.NODE_ENV === 'development' ? error.stack : undefined,
			}),
		};
	}
};

================
File: main.py
================
#!/usr/bin/env python
import sys
import warnings

from datetime import datetime

from tribe.crew import Tribe

warnings.filterwarnings("ignore", category=SyntaxWarning, module="pysbd")

# This main file is intended to be a way for you to run your
# crew locally, so refrain from adding unnecessary logic into this file.
# Replace with inputs you want to test with, it will automatically
# interpolate any tasks and agents information

def run():
    """
    Run the crew.
    """
    inputs = {
        'topic': 'AI LLMs',
        'current_year': str(datetime.now().year)
    }
    
    try:
        Tribe().crew().kickoff(inputs=inputs)
    except Exception as e:
        raise Exception(f"An error occurred while running the crew: {e}")


def train():
    """
    Train the crew for a given number of iterations.
    """
    inputs = {
        "topic": "AI LLMs"
    }
    try:
        Tribe().crew().train(n_iterations=int(sys.argv[1]), filename=sys.argv[2], inputs=inputs)

    except Exception as e:
        raise Exception(f"An error occurred while training the crew: {e}")

def replay():
    """
    Replay the crew execution from a specific task.
    """
    try:
        Tribe().crew().replay(task_id=sys.argv[1])

    except Exception as e:
        raise Exception(f"An error occurred while replaying the crew: {e}")

def test():
    """
    Test the crew execution and returns the results.
    """
    inputs = {
        "topic": "AI LLMs"
    }
    try:
        Tribe().crew().test(n_iterations=int(sys.argv[1]), inputs=inputs)

    except Exception as e:
        raise Exception(f"An error occurred while testing the crew: {e}")

================
File: pyproject.toml
================
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "tribe"
version = "0.1.0"
description = "An AI agent collaboration framework built on top of crewAI"
authors = [
    { name = "Your Name", email = "your.email@example.com" }
]
requires-python = ">=3.10,<3.13"
dependencies = [
    "crewai[tools]>=0.100.1,<1.0.0"
]

[project.scripts]
tribe = "tribe.main:main"
run_crew = "tribe.main:run_crew"
train = "tribe.main:train"
replay = "tribe.main:replay"
test = "tribe.main:test"

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.hatch.build]
include = [
    "tribe/**/*.py",
    "tribe/**/*.json",
    "tribe/**/*.txt",
    "pyproject.toml",
    "README.md"
]

[tool.hatch.metadata]
allow-direct-references = true

[tool.crewai]
type = "crew"

================
File: requirements.txt
================
crewai>=0.11.0
pydantic>=2.0.0
typing-extensions>=4.0.0
langchain>=0.1.0



================================================================
End of Codebase
================================================================
